\documentclass[10pt,a4paper]{article}
\usepackage{amsmath,amssymb,bm,graphicx,pictex,makeidx,subfigure}
\usepackage[italian,english]{babel}
\usepackage[center,small]{caption}[2007/01/07]
\usepackage{fancyhdr}
\usepackage{color}

\definecolor{blu}{rgb}{0,0,1}
\definecolor{verde}{rgb}{0,1,0}
\definecolor{rosso}{rgb}{1,0,0}

\begin{document}

\section{The $\chi^2$ in the presence of correlations}

Suppose to have an ensamble of $n$ measurements having the following
structure:
\begin{equation}
  m_i\pm \sigma_{i,\rm stat} \pm \sigma_{i,\rm unc} \pm \sigma_{i,\rm
    corr}^{(1)}\pm\dots \pm \sigma_{i,\rm
    corr}^{(k)}\,,
\end{equation}
where $m_i$, with $i=1,\dots, n$, is the central value of the $i$-th
measurement, $\sigma_{i,\rm stat}$ its (uncorrelated) statistical
uncertainty, $\sigma_{i,\rm unc}$ its uncorrelated systematic
uncertainty\footnote{There could be more than one uncorrelated
  systematic uncertainty. In this case, $\sigma_{i,\rm unc}$ is just
  the square root of the sum in quadrature of all the uncorrelated
  systematic uncertainties.}, and $\sigma_{i,\rm corr}^{(l)}$, with
$l=1,\dots,k$, its correlated systematic uncertainties. With this
information at hand, one can construct the full covariance matrix
$V_{ij}$ as follows (see for example Ref.~\cite{Ball:2012wy}):
\begin{equation}\label{eq:covmat}
  V_{ij}=\left(\sigma_{i,\rm stat}^2 +\sigma_{i,\rm unc}^2\right)\delta_{ij} + \sum_{l=1}^{k}\sigma_{i,\rm
    corr}^{(l)}\sigma_{j,\rm
    corr}^{(l)}\,.
\end{equation}
This is a clearly symmetric matrix. Given a set of predictions $t_i$
corresponding to the $n$ measurements of the ensamble, the $\chi^2$
takes the form:
\begin{equation}\label{eq:chi2cov}
  \chi^2=
  \sum_{i,j=1}^{n}\left(m_i-t_i\right)V_{ij}^{-1}\left(m_j-t_j\right) =
  \mathbf{y}^{T} \cdot \mathbf{V}^{-1} \cdot \mathbf{y}\,,
\end{equation}
where in the second equality we have used the matricial notation and
defined $y_i = m_i-t_i$. A convenient way to compute the $\chi^2$
relies on the Cholesky decomposition of the covariance matrix
$\mathbf{V}$. In particular, it can be proven that any symmetric and
positive definite matrix $\mathbf{V}$ can be decomposed as:
\begin{equation}\label{eq:choleskydec}
\mathbf{V} = \mathbf{L}\cdot\mathbf{L}^{T}\,,
\end{equation}
where $\mathbf{L}$ is a lower triangular matrix whose entries are
related recursively to those of $\mathbf{V}$ as follows:
\begin{equation}\label{eq:cholalg}
\begin{array}{rcl}
  L_{kk} &=&\displaystyle \sqrt{V_{kk}-\sum_{j=1}^{k-1}L_{kj}^2}\,,\\
  \\
  L_{ik} &=&\displaystyle
             \frac{1}{L_{kk}}\left(V_{ik}-\sum_{j=1}^{k-1}L_{ij}L_{kj}\right)\,,\quad
             k < i\,,\\
\\
  L_{ik} &=&\displaystyle 0\,,\quad
             k > i\,.\\
\end{array}
\end{equation}
It is then easy to see that the $\chi^2$ can be written as:
\begin{equation}
\chi^2 = \left|\mathbf{L}^{-1}\cdot \mathbf{y}\right|^2\,.
\end{equation}
But the vector $\mathbf{x} \equiv \mathbf{L}^{-1}\cdot \mathbf{y}$ is
the solution of the linear system:
\begin{equation}
  \mathbf{L} \cdot \mathbf{x} = \mathbf{y}\,,
\end{equation}
that can be efficiently solved by forward substitution, so that:
\begin{equation}
  \chi^2 = \left|\mathbf{x}\right|^2\,.
\end{equation}
Following this procedure, one does not need to compute explicitly the
inverse of the covariance matrix $\mathbf{V}$, simplifying
significantly the computation of the $\chi^2$.

\section{Additive and multiplicative uncertainties}

The correlated systematic uncertainties $\sigma_{i,\rm corr}^{(l)}$
may be either \textit{additive} or \textit{multiplicative}. The nature
of the single uncertainties is typically provided by the experiments
that release the measurements. A typical example of multiplicative
uncertainty is the luminosity uncertainty but there can be others.

Now let us express all the correlated systematic uncertainties
$\sigma_{i,\rm corr}^{(l)}$ as relative to the associate central value
$m_i$, so that we define\footnote{Note that this redefinition does not
  change the nature of the uncertainties, additive uncertainties
  remain additive as well as multiplicative uncertainties remain
  multiplicative.}:
\begin{equation}
\sigma_{i,\rm corr}^{(l)}\equiv  \delta_{i,\rm corr}^{(l)} m_i
\end{equation}
and let us also define
$s_i^2\equiv \sigma_{i,\rm stat}^2 +\sigma_{i,\rm unc}^2$ so that
Eq.~(\ref{eq:covmat}) can be rewritten as:
\begin{equation}\label{eq:covmat2}
  V_{ij}=s_i^2\delta_{ij} + \left(\sum_{l=1}^{k}\delta_{i,\rm
    corr}^{(l)}\delta_{j,\rm
    corr}^{(l)}\right)m_im_j\,.
\end{equation}
Now we split the correlated systematic uncertainties into $k_a$
additive uncertainties and $k_m$ multiplicative uncertainties, such
that $k_a+k_m=k$. This way Eq.~(\ref{eq:covmat2}) takes the form:
\begin{equation}\label{eq:covmat3}
  V_{ij}=s_i^2\delta_{ij} + \left(\sum_{l=1}^{k_a}\delta_{i,\rm
    add}^{(l)}\delta_{j,\rm
    add}^{(l)}+\sum_{l=1}^{k_m}\delta_{i,\rm
    mult}^{(l)}\delta_{j,\rm
    mult}^{(l)}\right)m_im_j\,.
\end{equation}
It is well known that this definition of the covariance matrix is
problematic in that it results in the so-called D'Agostini bias of the
multiplicative uncertainties~\cite{DAgostini:1993arp}. A possible
solution to this problem is the so-called
$t_0$-prescription~\cite{Ball:2009qv}, where the experimental central
value $m_i$ in the multiplicative term is replaced by a fixed
theoretical predictions $t_i^{(0)}$, typically computed in a previous
fit in which the ``standard'' definition of the covariance matrix in
Eq.~(\ref{eq:covmat}) (often referred to as \textit{experimental}
definition) is used. Applying the $t_0$ prescription, the covariance
matrix takes the form:
\begin{equation}\label{eq:covmat4}
  V_{ij}=s_i^2\delta_{ij} + \sum_{l=1}^{k_a}\delta_{i,\rm
    add}^{(l)}\delta_{j,\rm
    add}^{(l)}m_im_j+\sum_{l=1}^{k_m}\delta_{i,\rm
    mult}^{(l)}\delta_{j,\rm
    mult}^{(l)}t_i^{(0)}t_j^{(0)}\,.
\end{equation}

\section{Artificial generation of correlated systematics}

In order to implement the definition of the $\chi^2$ discussed above,
it is necessary to have the experimental information in terms of the
correlated systematic uncertainties $\sigma_{i,\rm corr}^{(l)}$. This
is what the experimental collaborations usually release. However, in
some cases this information is given in terms of a covariance
matrix. Therefore, one needs to find a workaround to generate
correlated systematic uncertainties out of a covariance matrix. 

Given a $n \times n$ symmetric matrix $\mathbf{C}$, it will have $n$
orthonormal eigenvectors $\mathbf{x}^{(i)}$, such that
$\mathbf{x}^{(i)}\cdot \mathbf{x}^{(j)}=\delta_{ij}$, each of which
will have a non-negative eigenvalue $\lambda_i$ associated:
\begin{equation}
  \mathbf{C}\cdot \mathbf{x}^{(i)} = \lambda_i \mathbf{x}^{(i)}\,, \quad
  i =1,\dots,n\,.
\end{equation}
If we define:
\begin{equation}\label{eq:artsys}
  \sigma_{i,\rm corr}^{(l)} = \sqrt{\lambda_l} x_i^{(l)}\,,\quad i,l=1,\dots,n\,,
\end{equation}
one can show that:
\begin{equation}\label{eq:artsydef}
  \sum_{l=1}^{n}\sigma_{i,\rm corr}^{(l)}\sigma_{j,\rm corr}^{(l)} =C_{ij}\,.
\end{equation}
To prove this equality we start from the following matricial relation:
\begin{equation}
\mathbf{C} =\mathbf{Q}\cdot \mathbf{\Lambda}\cdot \mathbf{Q}^{-1}\,,
\end{equation}
where $\mathbf{\Lambda}$ is a diagonal matrix with the eigenvalues
$\lambda_i$ on the diagonal ($\Lambda_{ij} = \lambda_i\delta_{ij}$),
while $\mathbf{Q}$ is a matrix whose columns are the eigenvectors
$\mathbf{x}^{(i)}$ ($Q_{ij} = x_{i}^{(j)}$). In addition, since in
this particular case
$\mathbf{x}^{(i)}\cdot \mathbf{x}^{(j)}=\delta_{ij}$, this implies
that:
\begin{equation}
\mathbf{Q}^T \cdot \mathbf{Q}  = \mathbf{I}\quad\Rightarrow\quad
\mathbf{Q}^{-1} =  \mathbf{Q}^{T}\,,
\end{equation}
so that:
\begin{equation}\label{eq:diagC}
  \mathbf{C} =\mathbf{Q}\cdot \mathbf{\Lambda}\cdot \mathbf{Q}^{T}\,.
\end{equation}
It follows that:
\begin{equation}
  C_{ij} =\sum_{k,l=1}^{n}Q_{ik}\Lambda_{kl} Q_{jl} = \sum_{k,l=1}^{n} x_{i}^{(k)}
  \lambda_{k}\delta_{kl}x_{j}^{(l)} = \sum_{l=1}^{n} \lambda_{l}x_{i}^{(l)}
  x_{j}^{(l)}=\sum_{l=1}^{n}\sigma_{i,\rm corr}^{(l)}\sigma_{j,\rm corr}^{(l)} \,,
\end{equation}
as required.

The matrix \textbf{C} can be regarded as the correlated contribution to
the full covariance matrix \textbf{V}. In particular, considering
Eqs.~(\ref{eq:covmat}) and~(\ref{eq:covmat2}), one can write:
\begin{equation}
\mathbf{V} = \mathbf{U} + \mathbf{C}\,,
\end{equation}
where $\mathbf{U}$ is a diagonal matrix of uncorrelated uncertainties:
\begin{equation}
U_{ij} = s_i^2\delta_{ij}\,.
\end{equation}
This defines the matrix $\mathbf{C}$ as:
\begin{equation}\label{eq:corrdef}
\mathbf{C} = \mathbf{V} - \mathbf{U}\,,
\end{equation}
such that, given a $n\times n$ covariance matrix $\mathbf{V}$ along
with its uncorrelated contribution $\mathbf{U}$, one can generate a
set of $n$ \textit{artificial} correlated systematics according to
Eq.~(\ref{eq:artsys}), where $\mathbf{C}$ is given in
Eq.~(\ref{eq:corrdef}), for each of the $n$ measurements. This allows
us to implement Eq.~(\ref{eq:covmat4}) for the construction of the
covariance matrix.

\section{Determining the systematic shifts}

In order to visualise the effect of systematic uncertainties, it is
instructive to compute the \textit{systematic shift} generated by the
systematic uncertainties. To do so, we need to write the $\chi^2$ in
terms of the so-called ``nuisance parameters'' $\lambda_\alpha$. One
can show that the definition of the $\chi^2$ in Eq.~(\ref{eq:chi2cov})
is equivalent to~\cite{Ball:2012wy}:
\begin{equation}\label{eq:chi2nuis}
\chi^2 = \sum_{i=1}^n\frac{1}{s_i^2}\left(m_i -t_i
  -\sum_{\alpha=1}^k\lambda_\alpha \sigma_{i,\rm corr}^{(\alpha)} \right)^2 + \sum_{\alpha=1}^k\lambda_\alpha^2\,.
\end{equation}
The optimal value of the nuisance parameters can be computed by
minimising the $\chi^2$ with respect to them, that is imposing:
\begin{equation}
\frac{\partial \chi^2}{\partial \lambda_\beta} = 0\,.
\end{equation}
This yields the system:
\begin{equation}\label{eq:nuissys}
  \sum_{\beta=1}^kA_{\alpha\beta}\lambda_\beta =\rho_\alpha\,,
\end{equation}
with:
\begin{equation}\label{eq:sysing}
A_{\alpha\beta}= \delta_{\alpha\beta}+\sum_{i=1}^n\frac{\sigma_{i,\rm corr}^{(\alpha)}\sigma_{i,\rm corr}^{(\beta)}}{s_i^2}\quad\mbox{and}\quad \rho_\alpha=\sum_{i=1}^n\frac{m_i-t_i}{s_i^2}\sigma_{i,\rm corr}^{(\alpha)}\,,
\end{equation}
that determines the values of $\lambda_\beta$. The quantity:
\begin{equation}
d_i =\sum_{\alpha=1}^k\lambda_\alpha \sigma_{i,\rm corr}^{(\alpha)}
\end{equation}
in Eq.~(\ref{eq:chi2nuis}) can be interpreted as a shift caused by the
correlated systematic uncertainties. Defining the shifted predictions
as:
\begin{equation}
\overline{t}_i =t_i+d_i\,,
\end{equation}
the $\chi^2$ reads:
\begin{equation}\label{eq:chi2nuisshift}
  \chi^2 = \sum_{i=1}^n\left(\frac{m_i -\overline{t}_i}{s_i}\right)^2 + \sum_{\alpha=1}^k\lambda_\alpha^2\,.
\end{equation}
Therefore, up to a penalty term given by the sum of the square of the
nuisance parameters, the $\chi^2$ takes the form of the uncorrelated
definition. In order to achieve a visual assessment of the agreement
between data and theory, it appears natural to compare the central
experimental values $m_i$ to the shifted theoretical predictions
$\overline{t}_i$ in units of the uncorrelated uncertainty $s_i$.

\section{Effect of cuts on the $\chi^2$}

A relevant question to ask is what is the effect of possible cuts on
the input dataset on the form of the $\chi^2$ in the presence of
correlations. In order to address this question, we consider a simple
dataset made of two datapoints equipped with a single uncorrelated
uncertainty and a single fully-correlated uncertainty:\footnote{Note
  that we need to include \textit{also} an uncorrelated uncertainty
  because otherwise the covariance matrix would be singular. This a
  symptom of the fact that if no uncorrelated uncertainties are
  present, the two points would be totally dependent on each
  other. This observation will be relevant in the following.}
\begin{equation}\label{eq:fullset}
S:\quad\{m_i\pm \sigma_i^u\pm \sigma_i^c\}\,,\quad i  =1,2\,.
\end{equation}
The resulting covariance matrix reads:
\begin{equation}
\mathbf{V}=\begin{pmatrix}
(\sigma_1^u)^2+ (\sigma_1^c)^2 & \sigma_1^c \sigma_2^c \\
\sigma_1^c \sigma_2^c & (\sigma_2^u)^2+ (\sigma_2^c)^2
\end{pmatrix}\,,
\end{equation}
with inverse:
\begin{equation}
\mathbf{V}^{-1}=\frac{1}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\begin{pmatrix}
(\sigma_2^u)^2+ (\sigma_2^c)^2 & -\sigma_1^c \sigma_2^c \\
-\sigma_1^c \sigma_2^c & (\sigma_1^u)^2+ (\sigma_1^c)^2
\end{pmatrix}\,.
\end{equation}
In addition, we a have a set of two theoretical predictions $\{t_i\}$
that allows us to define a column vector of residuals
$\mathbf{r}^{\rm T}=(r_1,\;r_2)$ with $r_i=m_i-t_i$. The $\chi^2$ then takes
the following explicit expression:
\begin{equation}\label{eq:fullchi2}
  \chi^2= \mathbf{r}^{\rm T}\cdot \mathbf{V}^{-1}\cdot \mathbf{r}=
  \frac{r_1^2\left[(\sigma_2^u)^2+ (\sigma_2^c)^2\right]+r_2^2\left[(\sigma_1^u)^2+ (\sigma_1^c)^2\right]-2 r_1 r_2\sigma_1^c \sigma_2^c}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\,.
\end{equation}
Now, suppose we want to exclude the datapoint with $i=2$ from the
definition of the $\chi^2$. The possibly most natural way to proceed
is to exclude the point, along with its uncertainties, directly from
the set in Eq.~(\ref{eq:fullset}). This straightforwardly leads to the
following $\chi^2$:
\begin{equation}\label{eq:redchi21}
{\chi}_{\rm cut\;1}^2 = \frac{r_1^2}{(\sigma_1^u)^2+ (\sigma_1^c)^2}\,.
\end{equation}
However, this procedure might be questioned in that it artificially
modifies the original dataset by effectively defining a \textit{new}
dataset where the point $i=2$ is not present. In the absence of
correlations this is a sound procedure because each single datapoint
can be regarder an independent subset of the original
set. However, this is not the case when correlations are present.

Therefore, it is necessary to devise a method that avoids any
modification of the dataset but yet allows one to prevent specific
datapoints to contribute to the $\chi^2$. In general, the purpose of
excluding datapoints from a fit is to avoid that the model used to
compute the predictions is forced outside its application region. One
can therefore \textit{enforce} that the model exactly agrees with the
datapoints to be excluded. This effectively amounts to set $t_2=m_2$,
or equivalently $r_2=0$, reducing Eq.~(\ref{eq:fullchi2}) to:
\begin{equation}\label{eq:fullchi2trunc}
  \chi_{\rm cut\;2}^2= 
  \frac{r_1^2\left[(\sigma_2^u)^2+ (\sigma_2^c)^2\right]}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\,.
\end{equation}
This has the clear advantage that the experimental information,
including correlations, remains totally intact. In order to see that
Eq.~(\ref{eq:fullchi2trunc}) has the right properties, we first take
the limit for $\sigma_2^c\rightarrow 0$. This gives:
\begin{equation}
  \lim_{\sigma_2^c\rightarrow 0}\chi_{\rm cut}^2= 
  \frac{r_1^2}{(\sigma_1^u)^2+ (\sigma_1^c)^2}\,,
\end{equation}
which reproduces the result of Eq.~(\ref{eq:redchi21}). This was to be
expected because no reference to the point $i=2$ can remain after the
removal of its correlation to the point $i=1$. In addition, it is also
very instructive to take the limit $\sigma_2^u\rightarrow 0$
Eq.~(\ref{eq:fullchi2trunc}), which gives:
\begin{equation}
\lim_{\sigma_2^u\rightarrow 0}\chi_{\rm cut}^2= 
\frac{r_1^2}{ (\sigma_1^u)^2}\,.
\end{equation}
In this case, not only any reference to the point $i=2$ drops out, but
also the dependence of the $\chi^2$ on the correlation uncertainty of
the point $i=1$ disappears. Despite at first sight this looks
counterintuitive, it seems to be the correct behaviour. To see this,
we observe that, if the point $i=2$ has no uncorrelated uncertainty,
its fluctuations are totally driven by the fluctuations of the point
$i=1$. Therefore, the point $i=2$ is completely dependent on the point
$i=1$ and thus cannot give any contribution to the $\chi^2$. In
addition, the correlation uncertainty of the point $i=1$ must also be
irrelevant because, being $i=2$ totally correlated to $i=1$, any
correlation of $i=1$ to $i=2$ translates into a correlation to itself
and this cannot contribute to the $\chi^2$ either. Importantly, if we
take the limit $\sigma_1^u\rightarrow 0$, the $\chi^2$ diverges no
matter the value of $\sigma_1^c$. This is consistent with setting
$\sigma_1^u=\sigma_2^u$ since the beginning in Eq.~(\ref{eq:fullset})
which indeed gives a singular $\chi^2$ because the covariance matrix
is singular. Eq.~(\ref{eq:redchi21}) does not produce these features
hanging in favour of Eq.~(\ref{eq:fullchi2trunc}).

\section{Monte Carlo replica generation}

In this section we consider the Monte Carlo generation of articial
replicas. The aim is to obtain a formula for the generation of
replicas that gives rise to a figure of merit distributed correctly,
\textit{i.e.} that follows a $\chi^2$ distribution with as many
degrees of freedom as number of data points. Following
Ref.~\cite{wiki:xxx}, we require that the $\chi^2$, that in matricial
form can be expressed as:
\begin{equation}\label{eq:chi2matrix}
  \chi^2 = \mathbf{y}^{T}\cdot \mathbf{V}^{-1} \cdot \mathbf{y}\,,
\end{equation}
where $\mathbf{V}$ is the $n\times n$ covariance matrix, being $n$ the
number of datapoints, and $\mathbf{y}$ is a column vector with $n$
entries that can be expressed as:
\begin{equation}
y_j = f_j-m_j\,,
\end{equation}
with $m_j$ a given value (to be thought as the central value) and
$f_j$ its fluctuated value, is distributed like:
\begin{equation}\label{eq:wikipedia}
\chi^2\sim \sum_{i=1}^{n}z_i^2=|\mathbf{z}|^2\,,
\end{equation}
where $z_1,\dots , z_n$ are independent, standard normal random
variables. As already discussed above, Eq.~(\ref{eq:chi2matrix}) can
alternatively be written as:
\begin{equation}\label{explicitchi2chol}
\chi^2 = \left|\mathbf{L}^{-1}\cdot \mathbf{y}\right|^2\,,
\end{equation}
where $\mathbf{L}$ is the Cholesky decomposition of the matrix
$\mathbf{V}$, such that:
\begin{equation}\label{eq:choleskydecagain}
\mathbf{V} = \mathbf{L}\cdot\mathbf{L}^{T}\,.
\end{equation}
Comparing Eqs.~(\ref{explicitchi2chol}) and~(\ref{eq:wikipedia}), one
can infer that the following relation should hold:
\begin{equation}
\mathbf{L}^{-1}\cdot \mathbf{y}\sim\mathbf{z}\,,
\end{equation}
which immediately implies:
\begin{equation}
\mathbf{y}\sim\mathbf{L}\cdot\mathbf{z}\,,
\end{equation}
or equivalently:
\begin{equation}\label{eq:deviations}
f_j\sim m_j+\sum_{i=1}^nL_{ji}z_i\,.
\end{equation}
In conclusion, in order for the $\chi^2$ to have the correct
distribution, the fluctuations $f_j$ around the experimental central
values $m_j$ of a given Monte Carlo replica have to be computed
according to Eq.~(\ref{eq:deviations}).

It is instructive to work out how Eq.~(\ref{eq:deviations}) relates to
more ``standard'' approaches to the generation of Monte Carlo replicas
such as those in Refs.~\cite{Ball:2008by, Ball:2014uwa}. The aim is to
find a closed-form expression for the Cholesky-decomposition matrix
$\mathbf{L}$ of the covariance matrix $\mathbf{V}$ in terms of the
single uncertainties. Through Eq.~(\ref{eq:deviations}), this will
finally allow us to derive a formula of the same kind of those given
in Refs.~\cite{Ball:2008by, Ball:2014uwa}. The basic observation is
that the covariance matrix can be written as:
\begin{equation}
\mathbf{V} = \mathbf{D}+ \sum_{l=1}^k 
\mathbf{V}^{(l)} = \sum_{l=0}^k \mathbf{L}^{(l)}\cdot \mathbf{L}^{(l)T}\,,
\end{equation}
with:
\begin{equation}
L_{ij}^{(l)}=\left\{
\begin{array}{ll}
s_i\delta_{ij}&\quad l = 0\,,\\
\sigma_i^{(l)}\delta_{j1}&\quad 1\leq l\leq k\,.
\end{array}
\right.
\end{equation}
On the other hand, we also know that:
\begin{equation}
  \mathbf{V} = \mathbf{L}\cdot \mathbf{L}^T\,,
\end{equation}
therefore:
\begin{equation}
  \mathbf{L}\cdot \mathbf{L}^T=\sum_{l=0}^k \mathbf{L}^{(l)}\cdot \mathbf{L}^{(l)T}\,.
\end{equation}
The question is to express $\mathbf{L}$ in terms of all the
$\mathbf{L}^{(l)}$. Unfortunately though this equation has no general
solution. However, a step forward can be done be exploiting the fact
that the matrices $\mathbf{V}^{(l)}$ are positive semidefinite
(\textit{i.e.} they have zero determinant) and thus their Cholesky
decomposition is not unique. In particular, provided that $k\leq n$,
one can choose:
\begin{equation}\label{eq:choldecomp2}
\widetilde{L}_{ij}^{(l)}=
\sigma_i^{(l)}\delta_{jl}\,,\quad 1\leq l\leq k\leq n\,,
\end{equation}
such that:
\begin{equation}
  \left(\sum_{l=1}^k\widetilde{\mathbf{L}}^{(l)}\right)\cdot\left(
    \sum_{l'=1}^k\widetilde{\mathbf{L}}^{(l')}\right)=\sum_{l=1}^k 
  \mathbf{V}^{(l)}\,.
\end{equation}
However, choosing:
\begin{equation}\label{eq:CholVapprox}
\widetilde{\mathbf{L}} = \mathbf{L}^{(0)} + \sum_{l=1}^k\widetilde{\mathbf{L}}^{(l)}\,,
\end{equation}
and multiplying it for its transpose would give:
\begin{equation}
\widetilde{\mathbf{L}}\cdot \widetilde{\mathbf{L}}^T = \mathbf{V} + \sum_{l=1}^k\left(\mathbf{L}^{(0)}\cdot \widetilde{\mathbf{L}}^{(l)T}+\widetilde{\mathbf{L}}^{(l)}\cdot \mathbf{L}^{(0)T}\right)\,,
\end{equation}
that gives back the covariance matrix plus a spurious term. Writing
the expression above in terms of single entries, one finds:
\begin{equation}
\sum_{m=1}^n\widetilde{L}_{im}\widetilde{L}_{jm} = V_{ij} + s_i \sigma_{j}^{(i)}+s_j\sigma_{i}^{(j)}\,.
\end{equation}
The peculiarity here is that the upper index of $\sigma_{j}^{(i)}$ and
$\sigma_{i}^{(j)}$, since $k\leq n$ (and in fact usually $k\ll n$,
\textit{i.e.} the number of correlated uncertainties is typically much
smaller than the number of data points), exceeds the allowed range. To
fix this problem we just assume $\sigma_{j}^{(i)}=0$ for $i>k$. This
finally gives:
\begin{equation}
\sum_{m=1}^n\widetilde{L}_{im}\widetilde{L}_{jm} = V_{ij}\,,\quad
\mbox{for } i,j>k\,,
\end{equation}
that means that, by adopting the Cholesky decomposition in
Eq.~(\ref{eq:choldecomp2}), part of the covariance matrix (usually the
largest part) is exactly recovered. Therefore,
$\widetilde{\mathbf{L}}$ Eq.~(\ref{eq:CholVapprox}) seems to be a
generally good approximator to the real Cholesky decomposition of the
covariance matrix $\mathbf{V}$. Plugging this equation into
Eq.~(\ref{eq:deviations}) immediately gives:
\begin{equation}\label{eq:deviations2}
  f_j\sim m_j+z_js_j + \sum_{l=1}^{k}z_l \sigma_{j,\rm
    corr}^{(l)}\,.
\end{equation}
It is interesting to focus on similarities and differences with the
formulas present in the literature. We will specifically consider
Eq.~(13) of Ref.~\cite{Ball:2008by} and Eq.~(20) of
Ref.~\cite{Ball:2014uwa}.

We start by considering the case in which there are only uncorrelated
and correlated additive uncertainties. In this case,
Refs.~\cite{Ball:2008by, Ball:2014uwa} are in mutual agreement and
only in partial agreement with our Eq.~(\ref{eq:deviations2}), the
difference being that the random numbers used to generate the
fluctuation associated to the $k$ correlated uncertainties are equal
to the first $k$ of the $n$ used for the uncorrelated uncertainties
(remember that we required $k\leq n$). Of course, if $k\ll n$, as it
often happens, the agreement between Eq.~(\ref{eq:deviations2}) and
Refs.~\cite{Ball:2008by, Ball:2014uwa} is asymptotically good.

When considering also correlated multiplicative uncertainties there
are differences across formulas of Refs.~\cite{Ball:2008by,
  Ball:2014uwa} and Eq.~(\ref{eq:deviations2}). While
Eq.~(\ref{eq:deviations2}) treats multiplicative uncertainties on the
same footing as additive uncertainties (consistently with the
construction of the covariance matrix in Eq.~(\ref{eq:covmat})),
Refs.~\cite{Ball:2008by, Ball:2014uwa} introduce a multiplicative
factor. In addition, Ref.~\cite{Ball:2008by} distinguishes between
relative and absolute multiplicative uncertainties.
\begin{figure}[h]
  \begin{centering}
    \includegraphics[width=0.49\textwidth]{plots/Chi2DistBABAR}
    \includegraphics[width=0.49\textwidth]{plots/Chi2DistALEPH}
    \caption{Distribution of the $\chi^2$'s of the Monte Carlo replica
      w.r.t. to the corresponding central values for BABAR (left) and
      ALEPH (right) experiments compared to the probability
      distribution function with to the appropriate number of degrees
      of freedom given by the number of data points. The blue
      histograms are produced using Eq.~(\ref{eq:deviations}), the red
      histogram is produced using Eq.~(13) of Ref.~\cite{Ball:2008by}
      where multiplicative uncertainties are treated as relative,
      while the green histogram is produced using Eq.~(20) of
      Ref.~\cite{Ball:2014uwa}. \label{fig:Chi2Dist}}
  \end{centering}
\end{figure}

\begin{thebibliography}{alp}

%\cite{Ball:2009qv}
\bibitem{Ball:2009qv}
  R.~D.~Ball {\it et al.} [NNPDF Collaboration],
  %``Fitting Parton Distribution Data with Multiplicative Normalization Uncertainties,''
  JHEP {\bf 1005} (2010) 075
  doi:10.1007/JHEP05(2010)075
  [arXiv:0912.2276 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP05(2010)075;%%
  %84 citations counted in INSPIRE as of 13 May 2018

%\cite{DAgostini:1993arp}
\bibitem{DAgostini:1993arp}
  G.~D'Agostini,
  %``On the use of the covariance matrix to fit correlated data,''
  Nucl.\ Instrum.\ Meth.\ A {\bf 346} (1994) 306.
  doi:10.1016/0168-9002(94)90719-6
  %%CITATION = doi:10.1016/0168-9002(94)90719-6;%%
  %146 citations counted in INSPIRE as of 13 May 2018

%\cite{Ball:2012wy}
\bibitem{Ball:2012wy}
  R.~D.~Ball {\it et al.},
  %``Parton Distribution Benchmarking with LHC Data,''
  JHEP {\bf 1304} (2013) 125
  doi:10.1007/JHEP04(2013)125
  [arXiv:1211.5142 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP04(2013)125;%%
  %102 citations counted in INSPIRE as of 13 May 2018

%\cite{Boughezal:2017nla}
\bibitem{Boughezal:2017nla}
  R.~Boughezal, A.~Guffanti, F.~Petriello and M.~Ubiali,
  %``The impact of the LHC Z-boson transverse momentum data on PDF determinations,''
  JHEP {\bf 1707} (2017) 130
  doi:10.1007/JHEP07(2017)130
  [arXiv:1705.00343 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP07(2017)130;%%
  %19 citations counted in INSPIRE as of 13 May 2018

\bibitem{wiki:xxx}
    Wikipedia contributors,
    Chi-square distribution --- {Wikipedia}{,} The Free Encyclopedia,
    2020.
   % \url{https://en.wikipedia.org/w/index.php?title=Chi-square_distribution&oldid=979585636}

%\cite{Ball:2008by}
\bibitem{Ball:2008by}
R.~D.~Ball \textit{et al.} [NNPDF],
%``A Determination of parton distributions with faithful uncertainty estimation,''
Nucl. Phys. B \textbf{809} (2009), 1-63
[erratum: Nucl. Phys. B \textbf{816} (2009), 293]
doi:10.1016/j.nuclphysb.2008.09.037
[arXiv:0808.1231 [hep-ph]].
%286 citations counted in INSPIRE as of 24 Oct 2020

%\cite{Ball:2014uwa}
\bibitem{Ball:2014uwa}
R.~D.~Ball \textit{et al.} [NNPDF],
%``Parton distributions for the LHC Run II,''
JHEP \textbf{04} (2015), 040
doi:10.1007/JHEP04(2015)040
[arXiv:1410.8849 [hep-ph]].
%2322 citations counted in INSPIRE as of 24 Oct 2020

\end{thebibliography}


\end{document}
