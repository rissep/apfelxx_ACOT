%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twoside,english]{paper}
\usepackage{lmodern}
\renewcommand{\ttdefault}{lmodern}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=2.5cm,lmargin=2cm,rmargin=2cm}
\usepackage{color}
\usepackage{babel}
\usepackage{float}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}
\usepackage{breakurl}
\usepackage{makeidx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{babel}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\usepackage{listings}

\begin{document}

\title{Interpolation}

\maketitle

\tableofcontents{}

\vspace{20pt}

In this part of the documentation we address the question of
interpolation. Interpolation is an extremely powerful tool that allows
one to reconstruct, within some accuracy, continuos functions when
knowing them only in a finite number of points. Unsurprisingly,
interpolation techniques are massively used in numerical codes that
deal with physical quantities that are continuos functions of
space-time variables, such as position and momentum. In this respect,
{\tt APFEL++} makes no exception.

In order to make the best of interpolation, {\tt APFEL++} has been
designed around a specific choice of the interpolation
strategy. Specifically, the very computational core of {\tt APFEL++}
relies on \textit{Langrange} polynomials and their properties. Despite
Langrange polynomials do not enjoy the smoothness of splines, they
enjoy a number of very useful properties that allow extending the use
of Lagrange polynomials from just interpolation to derivation and
integration. In the following, the interpolation strategy is detailed
along with its implementation and applications {\tt APFEL++}.

\section{Lagrange interpolation}\label{sec:LagrangeInterpolation}

In this section we will derive a general expression for the Lagrange
interpolating functions $w$. These functions are ubiquitous in {\tt
  APFEL++} in that they are used, not only for simple interpolations,
but also for computing convolutions, integrals in general, and also
derivatives. It is then important to give them a thorough derivation
in terms of Lagrange polynomial and to understand how they behave upon
derivation and integration.

Suppose one wants to interpolate the test function $g$ in the point
$x$ using a set of Lagrange polynomials of degree $k$. This requires a
subset of $k+1$ consecutive points on an interpolation grid, say
$\{x_{\alpha},\dots,x_{\alpha+k}\}$. The relative position between the
point $x$ and the subset of points used for the interpolation is
arbitrary. It is convenient to choose the subset of points such that
$x_\alpha < x \leq x_{\alpha+k}$.\footnote{Actually, it is not
  necessary to impose the constraint $x_\alpha < x \leq x_{\alpha+k}$.
  In case this relation is not fulfilled one usually speaks about
  \textit{extrapolation} rather than \textit{interpolation}. If not
  necessary, this option is typically not convenient because it may
  lead to a substantial deterioration in the accuracy with which
  $g(x)$ is determined.}  However, the ambiguity remains because there
are $k$ possible choices according to whether
$x_\alpha < x \leq x_{\alpha+1}$, or
$x_{\alpha+1} < x \leq x_{\alpha+2}$, and so on. For now we assume
that:
\begin{equation}
x_{\alpha} < x \leq x_{\alpha+1}\,,
\label{eq:assumption1}
\end{equation}
but we will release this assumption below. Using the standard Lagrange
interpolation procedure, one can approximate the function $g$ in $x$
as:
\begin{equation}
g(x) = \sum_{i=0}^k\ell_i^{(k)}(x)g(x_{\alpha+i})\,,
\label{particularCase}
\end{equation}
where $\ell_i^{(k)}$ is the $i$-th Lagrange polynomial of degree $k$
which can be written as:
\begin{equation}
\ell_i^{(k)}(x) = \prod^{k}_{m=0\atop m\ne
i}\frac{x-x_{\alpha+m}}{x_{\alpha+i}-x_{\alpha+m}}\,.
\label{eq:LagPoly}
\end{equation}
Since we have assumed that $x_\alpha < x \leq x_{\alpha+1}$ (see
Eq.~(\ref{eq:assumption1})), Eq.~(\ref{particularCase}) can be written
as:
\begin{equation}
  g(x) =
  \theta(x-x_{\alpha})\theta(x_{\alpha+1}-x)\sum_{i=0}^k
  g(x_{\alpha+i})\prod^{k}_{m=0\atop m\ne
    i}\frac{x-x_{\alpha+m}}{x_{\alpha+i}-x_{\alpha+m}}\,.
\label{particularCaseTheta}
\end{equation}

In order to make Eq.~(\ref{particularCaseTheta}) valid for all values
of $\alpha$, one just has to sum over all $N_x+1$ nodes of the
\textit{global} interpolation grid $\{x_0,\dots,x_{N_x}\}$, that is:
\begin{equation}
  g(x) =
  \sum_{\alpha=0}^{N_x-1}\theta(x-x_{\alpha})\theta(x_{\alpha+1}-x)\sum_{i=0}^k
  g(x_{\alpha+i})\prod^{k}_{m=0\atop m\ne i}\frac{x-x_{\alpha+m}}{x_{\alpha+i}-x_{\alpha+m}}\,.
\label{generalCase}
\end{equation}
Defining $\beta=\alpha+i$, one can rearrange the equation above as:
\begin{equation}
  g(x) =
  \sum_{\beta=0}^{N_x+k-1}w_\beta^{(k)}(x) g(x_{\beta})\,,
\label{generalCase2}
\end{equation}
that leads to the definition of the interpolating functions:
\begin{equation}
  w_\beta^{(k)}(x) = \sum_{i=0\atop i\leq\beta}^k
  \theta(x-x_{\beta-i})\theta(x_{\beta-i+1}-x) \prod^{k}_{m=0\atop m\ne i}\frac{x-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\,.
\label{eq:intfunc}
\end{equation}
Notice that the condition $i\leq\beta$ comes from the condition
$\alpha\geq 0$. It is important to observe that the sum in
Eq.~(\ref{generalCase2}) extends up to the $(N_x+k-1)$-th
node. Therefore, the original grid needs to be extended by $k-1$
nodes. However, the range of validity of the interpolation remains
that defined by the original grid, \textit{i.e.}
$x_0 \leq x \leq x_{N_x}$.

When implementing this interpolation procedure, it is important to
realise that, typically, only a small number of terms in the sum in
Eq.~(\ref{generalCase2}) is different from zero. For any given value
of $x$, it is possible to determine the values of the index $\beta$
for which the interpolating functions $w_\beta^{(k)}$ are different
from zero, reducing (often dramatically) the amount of sums required
to carry out an interpolation. The range of $\beta$ is easily
determined by observing that in Eq.~(\ref{particularCaseTheta}) the
summation extends on the nodes between $x_\alpha$ and
$x_{\alpha+k}$. But since $\beta$ is defined like $\alpha+i$ this
exactly defines the range in $\beta$:
\begin{equation}
\alpha(x) \leq \beta \leq \alpha(x) + k\,,
\end{equation}
where the function $\alpha(x)$ is implicitly defined through
Eq.~(\ref{eq:assumption1}). Therefore, Eq.~(\ref{generalCase2})
becomes:
\begin{equation}
  g(x) =
  \sum_{\beta=\alpha(x)}^{\alpha(x)+k}w_\beta^{(k)}(x) g(x_{\beta})\,,
\label{eq:limitedsum}
\end{equation}

Since the interpolation functions $w_\beta^{(k)}(x)$ often appear
inside integrals, it is very useful to use the fact that they are
different from zero only over a limited interval, specifically:
\begin{equation}
w_\beta^{(k)}(x) \neq 0\quad \Leftrightarrow\quad
x_{\beta-k}<x < x_{\beta+1}\,.
\label{eq:limits}
\end{equation}
This allows one to optimise the integration restricting the
integration region only to where the interpolating functions are
different from zero.

\subsection{Generalised interpolation}

In most of the applications within {\tt APFEL++} the assumption in
Eq.~(\ref{eq:assumption1}) is used. However, sometimes it may be
useful to release this assumption. A situation in which this is
advantageous is in the presence of non-smooth or discontinuos
functions (such as PDFs and FFs as function of the factorisation scale
$\mu$ in correspondence of the heavy-quark thresholds). When
interpolating these functions one should not interpolate over the
discontinuities. To do so and yet retain a given interpolation degree,
one can release the assumption in
Eq.~(\ref{eq:assumption1}). Specifically, we generalise it to:
\begin{equation}
  x_{\alpha+t} < x \leq
  x_{\alpha+t+1}\quad\mbox{with}\quad t = 0,\dots,k-1\,,
\label{IntAssumptionGen}
\end{equation}
such that the interpolation formula becomes:
\begin{equation}
  g(x) =
  \sum_{\alpha=-t}^{N_x-t-1}\theta(x-x_{\alpha+t})\theta(x_{\alpha+t+1}-x)\sum_{i=0}^k
  g(x_{\alpha+i})\prod^{k}_{m=0\atop m\ne i}\frac{x-x_{\alpha+m}}{x_{\alpha+i}-x_{\alpha+m}}\,,
\label{MoreGeneralCase}
\end{equation}
that can be rearranged as:
\begin{equation}
g(x) =
\sum_{\beta=-t}^{N_x+k-t-1}w_{\beta,t}^{(k)}(x) g(x_{\beta})\,,
\label{generalCase3}
\end{equation}
with:
\begin{equation}
w_{\beta,t}^{(k)}(x) = \sum_{i=0,i\leq\beta}^k
\theta(x-x_{\beta-i+t})\theta(x_{\beta-i+t+1}-x) \prod^{k}_{m=0,m\ne
i}\frac{x-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\,,
\label{eq:generalisedintfuncs}
\end{equation}
being the ``generalised'' interpolation functions. We observe that the
support region of $w_{\beta,t}^{(k)}$ is:
\begin{equation}
w_{\beta,t}^{(k)}(x)\neq 0 \quad\Leftrightarrow\quad x_{\beta+t-k} < x < x_{\beta+t+1}\,,
\end{equation}
that generalises Eq.~(\ref{eq:limits}). The generalised interpolation
functions can be used to avoid interpolating over some particular grid
nodes. In order to avoid interpolation over a specific node of the
grid, one can choose $t$ dynamically in such a way that $\beta+t$ in
Eq.~(\ref{eq:generalisedintfuncs}) never corresponds to that
particular node. This mechanism is implemented in {\tt APFEL++} as
follows. The interpolation grid is chosen to have two nodes in
correspondence of the \textit{threshold} $x_T$, but slightly displaced
  up and down by an ``infinitesimal'' amount $\epsilon$ to keep them
  separate, that is:
\begin{equation}
\{x_0,\dots,x_{\alpha_t-1},x_{\alpha_t},\dots,x_{N_x}\}\quad\mbox{with}\quad x_{\alpha_t-1}=x_T-\epsilon\quad\mbox{and}\quad x_{\alpha_t}=x_T+\epsilon\,.
\end{equation}
The aim is then to avoid interpolating over the nodes $x_{\alpha_t-1}$
and $x_{\alpha_t}$. By default we assume $t=0$ in
Eq.~(\ref{eq:generalisedintfuncs}) so that we automatically reduce to
Eq.~(\ref{eq:intfunc}). In this situation, we are assuming
Eq.~(\ref{eq:assumption1}) where effectively the index $\alpha$ is
determined dynamically according to the values of $x$. Therefore, we
can effectively write:
\begin{equation}
x_{\alpha(x)} < x \leq x_{\alpha(x)+1}\,,
\end{equation}
which implicitly defines the function
$\alpha(x)$. Eq.~(\ref{particularCaseTheta}) then requires summing
over the $k+1$ nodes of the grid
$\{x_{\alpha(x)},\dots,x_{\alpha(x)+k}\}$. However, when the point $x$
approaches $x_T$ from below, the range
$\{x_{\alpha(x)},\dots,x_{\alpha(x)+k}\}$ may end up enclosing both
nodes $x_{\alpha_t-1}$ and $x_{\alpha_t}$. To avoid this, we promote
the index $t$ in Eq.~(\ref{IntAssumptionGen}) to a function of $x$
defined through the inequalities:
\begin{equation}
\left\{\begin{array}{l}
x< x_T\,,\\
\\
x_{\alpha_t-2} < x_{\alpha(x)-t(x)+k}\leq x_{\alpha_t-1}\,,
\end{array}\right.
\end{equation}
that translate into:
\begin{equation}
\left\{\begin{array}{l}
\alpha(x) \leq \alpha_t -2\,,\\
\\
\alpha_t-2 <  \alpha(x)-t(x)+k \leq \alpha_t-1\,.
\end{array}\right.
\end{equation}
Imposing the unnecessary but convenient constraint $t(x)\geq 0$,
finally gives:
\begin{equation}
t(x) = \mbox{max}\left[\mbox{min}\left[\alpha(x),\alpha_t -2\right]
  -\alpha_t+k + 1, 0\right] \,,
\label{eq:tdef}
\end{equation}
that also obeys:
\begin{equation}
0\leq t(x) \leq k - 1\,,
\end{equation}
as required. In addition, as in Eq.~(\ref{eq:limitedsum}), the
summation over $\beta$ in Eq.~(\ref{generalCase3}) can be restricted
to a range of $k+1$ nodes as:
\begin{equation}
g(x) =
\sum_{\beta=\alpha(x)-t(x)}^{\alpha(x)-t(x)+k}w_{\beta,t}^{(k)}(x)
g(x_{\beta})\,.
\label{generalCase4}
\end{equation}

\subsection{Bi-dimensional interpolation}

As discussed in the section devoted to the computation of the
convolution integrals, the interpolation functions can be used to
compute integrals of the following kind:
\begin{equation}
I_1 = \int_{x_0}^{x_{N_x}}dx\,g(x)f(x)\,,
\end{equation}
where $f$ is a smooth function. Using Eqs.~(\ref{generalCase2})
and~(\ref{eq:limits}) we have that:
\begin{equation}
  I_1 = \sum_{\beta=0}^{N_x+k-1} W_\beta g(x_{\beta})\,,
\end{equation}
with:
\begin{equation}
W_\beta = \int_{x_{{\rm max}(0,\beta-k)}}^{x_{{\rm
      min}(N_x,\beta+1)}}dx \,w_\beta^{(k)}(x)f(x)\,.
\label{eq:monodim}
\end{equation}
The equation above can be easily generalised to a bidimensional
integral as:
\begin{equation}
I_2 = \int_{x_0}^{x_{N_x}}dx \int_{y_0}^{y_{N_y}}dy\,g(x,y)f(x,y) = \sum_{\alpha=0}^{N_x+k-1} \sum_{\beta=0}^{N_y+l-1} W_{\alpha\beta} g(x_{\alpha},y_{\beta})\,,
\end{equation}
with:
\begin{equation}
W_{\alpha\beta} = \int_{x_{{\rm max}(0,\alpha-k)}}^{x_{{\rm
      min}(N_x,\alpha+1)}}dx \int_{y_{{\rm max}(0,\beta-k)}}^{y_{{\rm
      min}(N_y,\beta+1)}}dy
\,w_\alpha^{(k)}(x)\,w_\beta^{(l)}(y)\,f(x,y)\,.
\label{eq:bidim}
\end{equation}
As discussed below in much detail, we mention that the functions $w$
are piecewise. In particular, while they are continuous in
correspondence of the nodes of the grid, their first derivative is
not. As a consequence, the result of the numerical integrals in
Eqs.~(\ref{eq:monodim}) and~(\ref{eq:bidim}) may be inaccurate. To
overcome this problem, it is sufficient to split the integrals in
sub-integrals over the intervals delimited by two consecutive
nodes. Using Eq.~(\ref{eq:limits}), it is easy to see that, for an
interpolation of degree $k$, one needs to compute $k+1$ integrals over
the intervals included between the $(\beta-k)$-th and the
$(\beta+1)$-th node.

\subsection{Derivative and integral of an interpolated function}

The simple form of the Lagrange polynomials allows, in some cases, for
the analytic handling of operations such has derivation and
integration of the interpolated functions. In this section we discuss
how derivation and integration can be carried out analytically
exploiting some specific properties of the Lagrange polynomials.

Referring Eq.~(\ref{particularCase}), the main observation is that the
interpolating functions $\ell_i^{(k)}$ are solutions of the following
differential-equation system:
\begin{equation}
\left\{
\begin{array}{l}
\displaystyle \frac{d\ell_i^{(k)}}{dx} = \left(\sum_{n=0 \atop n\neq
  i}^k\frac{1}{x-x_{\alpha+n}}\right) \ell_i^{(k)}(x)\\
\\
\ell_i^{(k)}(x_{\alpha+i}) = 1
\end{array}
\right.\,,
\label{eq:diffeqlagpol}
\end{equation}
whose solution is Eq.~(\ref{eq:LagPoly}). This allows us to compute the
derivative of the test function $g$ in Eq.~(\ref{eq:limitedsum}) only
knowing its values on the grid points as: 
\begin{equation}
  \frac{dg}{dx} =
  \sum_{\beta=\alpha(x)}^{\alpha(x)+k}\mathcal{D}_\beta^{(k)}(x)
  g(x_{\beta})\,,
\label{DerGeneralCase2}
\end{equation}
where:
\begin{equation}
  \mathcal{D}_\beta^{(k)}(x) = \sum_{i=0}^{{\rm min}(k,\beta)}
  \theta(x-x_{\beta-i})\theta(x_{\beta-i+1}-x) \left(\sum_{n=0 \atop n\neq
  i}^k\frac1{x_\beta-x_{\beta-i+n}}\prod^{k}_{m=0\atop m\ne
    i,n}\frac{x-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\right)\,.
\label{eq:derintfunc}
\end{equation}
It should be pointed out that, due to the discontinuity of the
derivative of the interpolation functions on the grid nodes, numerical
derivation through Eq.~(\ref{DerGeneralCase2}) does not work when $x$
coincides with a grid node. This problem can be overcome by simply
slightly displacing the value of $x$ in case in falls on a grid node.

Eq.~(\ref{eq:diffeqlagpol}) can also be used to compute integrals of
the function $g$. In particular, suppose we want to compute the
integral:
\begin{equation}
I(a,b) = \int_a^b dy\,g(y)\,.
\label{eq:IntegralInt}
\end{equation}
Using Eq.~(\ref{eq:limitedsum}), we find that:
\begin{equation}
  I(a,b)=
  \int_a^b dy
  \sum_{\beta=\alpha(y)}^{\alpha(y)+k}w_\beta^{(k)}(y)
  g(x_{\beta}) = \int_a^b dy\left[w_{\alpha(y)}^{(k)}(y)
  g(x_{\alpha(y)})+\dots+w_{\alpha(y)+k}^{(k)}(y)
  g(x_{\alpha(y)+k})\right]\,.
\end{equation}
The dependence on the integration variable $y$ of the index $\alpha$
complicates the solution of this integral. However, it is easy to
derive a close form for the function $\alpha(y)$:
\begin{equation}
\alpha(y) = -1 + \sum_{\beta = 0}^{N_x}\theta(y-x_\beta)\,,
\end{equation}
that (obviously) means that $\alpha(y)$ is constant on the separate
intervals $[x_0:x_1]$, $[x_1:x_2]$, and so on. This allows us to break
the integral above as follows:
\begin{equation}
\int_a^b dy \sum_{\beta=\alpha(y)}^{\alpha(y)+k}w_\beta^{(k)}(y)
  g(x_{\beta})= \left[\int_a^{x_{\alpha(a)+1}}+ \sum_{\gamma=\alpha(a)+1}^{\alpha(b)-1}\int_{x_\gamma}^{x_{\gamma+1}}+\int_{x_{\alpha(b)}}^b\right]dy \sum_{\beta=\alpha(y)}^{\alpha(y)+k}w_\beta^{(k)}(y)
  g(x_{\beta})\,,
\end{equation}
such that in each single integral the integrand has a constant value
of $\alpha$. The three terms above can be separately rearranged as:
\begin{equation}
\sum_{\beta=\alpha(a)}^{\alpha(a)+k}g(x_{\beta})\int_a^{x_{\alpha(a)+1}}dy\,w_\beta^{(k)}(y)\,,
\end{equation}
\begin{equation}
\sum_{\gamma=\alpha(a)+1}^{\alpha(b)-1}\sum_{\beta=\gamma}^{\gamma+k}g(x_{\beta})\int_{x_\gamma}^{x_{\gamma+1}}dy\,w_\beta^{(k)}(y)\,,
\end{equation}
\begin{equation}
\sum_{\beta=\alpha(b)}^{\alpha(b)+k}g(x_{\beta})\int_{x_{\alpha(b)}}^bdy\,
w_\beta^{(k)}(y)\,,
\end{equation}
where we have used the fact that $\alpha(x_\gamma)=\gamma$. Notice
that with this we managed to obtain direct integrations of the
interpolating functions $w_\beta^{(k)}$. Now we note that the three
terms above span the range of nodes
$\beta\in[\alpha(a):\alpha(b)+k]$. Therefore, the integral in
Eq.~(\ref{eq:IntegralInt}) can be written as:
\begin{equation}
  I(a,b) =
  \sum_{\beta=\alpha(a)}^{\alpha(b)+k}\mathcal{G}_\beta^{(k)}(a,b)
  g(x_{\beta})\,,
\label{eq:integralT0}
\end{equation}
with:
% \begin{equation}
% \mathcal{G}_\beta^{(k)}(a,b) = \int_a^bdy\,w_\beta^{(k)}(y)\,.
% \end{equation}
% From Eq.~(\ref{eq:limits}), we know that the integrand
% $w_\beta^{(k)}(y)$ is zero outside the interval
% $y\in [x_{\beta-k}:x_{\beta+1}]$. Assuming that $a\leq b$, there are a
% few possible configurations:
% \begin{itemize}
% \item $x_{\beta-k}\geq b$ or $x_{\beta+1}\leq a$: in these cases there is no
%   overlap between the support region of $w_\beta^{(k)}$ and
%   the integration range. It follows that:
% \begin{equation}
% \mathcal{G}_\beta^{(k)}(a,b)=0\,.
% \end{equation}

% \item $x_{\beta-k}\geq a$ and $x_{\beta+1}\leq b$: in this case the
%   support region of $w_\beta^{(k)}$ is entirely contained in
%   the integration range. Therefore:
% \begin{equation}
% \begin{array}{rcl}
% \displaystyle \mathcal{G}_\beta^{(k)}(a,b) =
%   \int_{x_{\beta-k}}^{x_{\beta+1}}dy\,w_\beta^{(k)}(y)&=&\displaystyle
%                                                                     \sum_{i=0}^{{\rm
%                                                                     min}(k,\beta)}
%   \int_{x_{\beta-k}}^{x_{\beta+1}}dy\,\theta(y-x_{\beta-i})\theta(x_{\beta-i+1}-y)
%   \prod^{k}_{m=0\atop m\ne
%     i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\\
% \\
% &=&\displaystyle \sum_{i=0}^{{\rm min}(k,\beta)}
%   \int_{x_{\beta-i}}^{x_{\beta-i+1}}dy
%   \prod^{k}_{m=0\atop m\ne
%     i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}
% \end{array}
% \end{equation}

% \item $x_{\beta-k}\geq a$ and $x_{\beta+1}> b$:
% \begin{equation}
% \mathcal{G}_\beta^{(k)}(a,b) = \int_{x_{\beta-k}}^{b}dy\,w_\beta^{(k)}(y)=\sum_{i=0}^{{\rm min}(k,\beta)}\theta(b-x_{\beta-i})
%   \int_{x_{\beta-i}}^{{\rm min}(x_{\beta-i+1},b)}dy
%   \prod^{k}_{m=0\atop m\ne
%     i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}
% \end{equation}

% \item $x_{\beta-k}< a$ and $x_{\beta+1}\leq b$:
% \begin{equation}
% \mathcal{G}_\beta^{(k)}(a,b) = \int_{a}^{x_{\beta+1}}dy\,w_\beta^{(k)}(y)=\sum_{i=0}^{{\rm min}(k,\beta)}
%   \theta(x_{\beta-i+1}-a)\int_{{\rm max}(x_{\beta-i},a)}^{x_{\beta-i+1}}dy
%   \prod^{k}_{m=0\atop m\ne
%     i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}
% \end{equation}

% \item $x_{\beta-k}< a$ and $x_{\beta+1}> b$: in this case the support
%   region of $w_\beta^{(k)}$ contains the integration
%   range. Therefore:
% \begin{equation}
% \mathcal{G}_\beta^{(k)}(a,b) = \int_{a}^{b}dy\,w_\beta^{(k)}(y) =\sum_{i=0}^{{\rm min}(k,\beta)}
%   \theta(b-x_{\beta-i})\theta(x_{\beta-i+1}-a)\int_{{\rm max}(x_{\beta-i},a)}^{{\rm min}(x_{\beta-i+1},b)}dy\prod^{k}_{m=0\atop m\ne
%     i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\,.
% \end{equation}

% \end{itemize}

% \newpage

\begin{equation}
\begin{array}{rcl}
\displaystyle \mathcal{G}_\beta^{(k)}(a,b) &=& \displaystyle
                                               \int_{{\rm
                                               max}(a,x_{\beta-k})}^{{\rm
                                               min}(b,x_{\beta+1})}
                                               dy\,w_\beta^{(k)}(y)\\
\\
&=&\displaystyle 
\sum_{i=0}^{{\rm min}(k,\beta)}
  \int_{{\rm max}(a,x_{\beta-k})}^{{\rm min}(b,x_{\beta+1})}dy\,\theta(y-x_{\beta-i})\theta(x_{\beta-i+1}-y)
  \prod^{k}_{m=0\atop m\ne
    i}\frac{y-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\,.
\end{array}
\end{equation}
Now, assuming that $a\leq b$, we can write the integral above as:
\begin{equation}
\int_{{\rm max}(a,x_{\beta-k})}^{{\rm min}(b,x_{\beta+1})} dy\,\theta(y-x_{\beta-i})\theta(x_{\beta-i+1}-y)\dots
=\theta(b-x_{\beta-i})\theta(x_{\beta-i+1}-a)\int_{{\rm max}(a,x_{\beta-i})}^{{\rm min}(b,x_{\beta-i+1})}dy\dots\,,
\end{equation}
so that:
\begin{equation}
\mathcal{G}_\beta^{(k)}(a,b) = \sum_{i=0}^{{\rm min}(k,\beta)}
  \theta(b-x_{\beta-i})\theta(x_{\beta-i+1}-a)\prod^{k}_{m=0\atop m\ne
    i}\frac{1}{x_{\beta}-x_{\beta-i+m}}\int_{{\rm max}(a,x_{\beta-i})}^{{\rm min}(b,x_{\beta-i+1})}dy\prod^{k}_{m=0\atop m\ne
    i}(y-x_{\beta-i+m})\,.
\end{equation}
It is easy to see that:
\begin{equation}
\prod^{k}_{m=0\atop m\ne
i}(y-x_{\beta-i+m})=\sum_{n=0}^k (-1)^n p_{\beta}^{(k)}(n)y^{k-n}\,,
\label{eq:expansion}
\end{equation}
such that:
\begin{equation}
\begin{array}{rcl}
\displaystyle \mathcal{G}_\beta^{(k)}(a,b) &=&\displaystyle  \sum_{i=0}^{{\rm min}(k,\beta)}
  \theta(b-x_{\beta-i})\theta(x_{\beta-i+1}-a)\\
\\
&\times&\displaystyle \left[\prod^{k}_{m=0\atop m\ne
    i}\frac{1}{x_{\beta}-x_{\beta-i+m}}\right]\sum_{n=0}^k
         \frac{(-1)^n p_{\beta}^{(k)}(n)}{k-n+1}
         \left(\overline{b}^{k-n+1}-\overline{a}^{k-n+1}\right)\,.
\end{array}
\label{eq:linIntegral}
\end{equation}
with $\overline{a} = {\rm max}(a,x_{\beta-i})$ and
$\overline{b} = {\rm min}(b,x_{\beta-i+1})$.
% For the moment we leave the integral unsolved, we will come back on
% that later because the situation is more complicated than that.
What is left to do is to determine the coefficients
$p_{\beta}^{(k)}$. For convenience, let us define the set
$\{r_1,\dots,r_k\}=\{x_{\beta-i},\dots,x_{\beta-1},x_{\beta+1},\dots,x_{\beta-i+k}\}$.\footnote{From
  the definition of the set $\{r_1,\dots,r_k\}$, it becomes clear that
  the subscript $\beta$ of the coefficients $p_{\beta}^{(k)}$ refers
  to the fact that this coefficients are computed on a vector of $k$
  nodes where the node $x_{\beta}$ has been removed.} The coefficients
$p_{\beta}^{(k)}$ defined in Eq.~(\ref{eq:expansion}) can be expressed
as:
\begin{equation}
  p_{\beta}^{(k)}(n)=\sum_{i_1=1}^k r_{i_1}\sum_{i_2=i_1+1}^k
  r_{i_2}\dots\sum_{i_n=i_{n-1}+1}^k r_{i_n}\,.
\label{eq:pncoef}
\end{equation}
In order to obtain a convenient algorithm to compute the coefficients
$p_{\beta}^{(k)}$, we define the vectorial function:
\begin{equation}
\mathbf{f}^{(k)}(\mathbf{r},\mathbf{a})\quad\mbox{with components}\quad  f_j^{(k)}\left(\mathbf{r},\mathbf{a}\right)=\sum_{i=j+1}^k r_{i}a_{i}\,.
\end{equation}
It is important to notice that while $\mathbf{r}$ is a $k$-dimensional
vector with index running between 1 and $k$, $\mathbf{a}$ and
$\mathbf{f}^{(k)}$ are in principle infinite-dimensional vectors with
index running between $-\infty$ and $+\infty$.  However, given the
definition of its components in Eq.~(\ref{eq:pncoef}), it turns out
that $f_j^{(k)}=0$ for $j\geq k$. The same applies to $\mathbf{a}$
because, as we will see below, it has to be identified with some
$\mathbf{f}^{(k)}$.  The function $\mathbf{f}^{(k)}$ can now be used
to define the vector $\mathbf{P}^{(k)}(n)$ recursively. The relevant
recursive relation is:
\begin{equation}
\mathbf{P}^{(k)}(n+1) = \mathbf{f}^{(k)}(\mathbf{r},\mathbf{P}^{(k)}(n))\quad\mbox{with}\quad\mathbf{P}^{(k)}(0) = \mathbf{1}\,.
\end{equation}
Finally, the coefficient $p_{\beta}^{(k)}(n)$ is the zero-th
component of the vector $\mathbf{P}^{(k)}(n)$, \textit{i.e.}
$p_{\beta}^{(k)}(n)\equiv P_0^{(k)}(n)$.

% As mentioned above, there is a complication. The discussion above
% applies to Lagrange interpolating functions written in terms of powers
% of $x$. Nonetheless, APFEL implements an interpolation procedure based
% on powers of $\ln x$. Specifically, the Lagrange polynomials look like
% this:
% \begin{equation}
%   \ell_i^{(k)}(x) = \prod^{k}_{m=0\atop m\ne
%     i}\frac{\ln x-\ln x_{\alpha+m}}{\ln x_{\alpha+i}-\ln
%     x_{\alpha+m}}\,.
%\label{eq:LagPolyLog}
% \end{equation}
% Therefore, deriving and integrating w.r.t. $x$ needs to account for
% the presence of the logarithm. This is not particularly complicated
% for the derivative because the derivation above applies verbatim to
% $dg/d\ln x$ and this easily translates into $dg/dx$ using the chain
% rule:
% \begin{equation}
% \frac{dg}{dx} = \frac{d\ln x}{dx}\frac{dg}{d\ln x} = \frac{1}{x} \frac{dg}{d\ln x}\,.
% \end{equation}

% The integration is instead more complicated because the integral in
% Eq.~(\ref{eq:linIntegral}) needs to be replaced as follows:
% \begin{equation}
% \int_{\overline{a}}^{\overline{b}}dy\,y^{k-n}\quad\rightarrow\quad \int_{\overline{a}}^{\overline{b}} dy\,\ln^{k-n}y=\sum_{p=0}^{k-n}\frac{(-1)^{k-n+p}(k-n)!}{p!}\left[\overline{b}\ln^p \overline{b}-\overline{a}\ln^p \overline{a}\right]\,.
% \end{equation}
% with $\overline{a} = {\rm max}(a,x_{\beta-i})$ and
% $\overline{b} = {\rm min}(b,x_{\beta-i+1})$.
% Therefore, after some manipulation, Eq.~(\ref{eq:linIntegral}) becomes:
% \begin{equation}
% \begin{array}{rcl}
% \displaystyle \mathcal{G}_\beta^{(k)}(a,b) &=&\displaystyle
%                                                \sum_{j=0}^{{\rm min}(\beta,k)}
%   \theta(b-x_{\beta-j})\theta(x_{\beta-j+1}-a)\\
% \\
% &\times&\displaystyle\left[\prod^{k}_{\delta=0\atop \delta\ne
%     j}\frac{1}{\ln x_{\beta}-\ln x_{\beta-j+\delta}}\right]\left[\sum_{p=0}^k q_\beta^{(k)}(p)\frac{(-1)^{k+p} \left[\overline{b}\ln^{p} \overline{b}-\overline{a}\ln^p \overline{a}\right]}{p!}\right]\,.
% \end{array}
% \label{eq:logIntegral}
% \end{equation}
% with:
% \begin{equation}
%   q_\beta^{(k)}(p) = \sum_{\eta=p}^{k}
%   \eta!\,p_{\beta}^{(k)}(k-\eta)\,.
% \end{equation}

Now, we turn to derive derivation and integration formulas for the
generalised interpolation formula in Eq.~(\ref{generalCase4}). The
derivative is as simple as in the $t=0$ case, that is:
\begin{equation}
\frac{dg}{dx} =
\sum_{\beta=\alpha(x)-t(x)}^{\alpha(x)-t(x)+k}\mathcal{D}_{\beta,t}^{(k)}(x) g(x_{\beta})\,,
\end{equation}
with:
\begin{equation}
\mathcal{D}_{\beta,t}^{(k)}(x) = \sum_{i=0}^{{\rm min}(k,\beta)}
\theta(x-x_{\beta-i+t(x)})\theta(x_{\beta-i+t(x)+1}-x) \left(\sum^{k}_{n=0\atop m\ne
i}\frac{1}{x_{\beta}-x_{\beta-i+n}}\prod^{k}_{m=0\atop m\ne
i,n}\frac{x-x_{\beta-i+m}}{x_{\beta}-x_{\beta-i+m}}\right)\,.
\end{equation}

Due to the presence of the function $t(x)$, defined in
Eq.~(\ref{eq:tdef}), the integration procedure is more involved. We
want to compute:
\begin{equation}
\begin{array}{rcl}
  I(a,b)&=&\displaystyle
  \int_a^b dy
  \sum_{\beta=\alpha(y)-t(y)}^{\alpha(y)-t(y)+k}w_{\beta,t}^{(k)}(y)
  g(x_{\beta}) \\
\\
&=&\displaystyle \left[\int_a^{x_{\alpha(a)+1}}+ \sum_{\gamma=\alpha(a)+1}^{\alpha(b)}\int_{x_\gamma}^{x_{\gamma+1}}-\int_b^{x_{\alpha(b)+1}}\right]dy \sum_{\beta=\alpha(y) -t(y)}^{\alpha(y)-t(y)+k}w_{\beta,t}^{(k)}(y)
  g(x_{\beta})\,.
\end{array}
\end{equation}
As in the case of $t=0$, each of the integrals above has a constant
value of $\alpha(y)$ and $t(y)$ so that sum over $\beta$ and the
integral sign can be exchanged:
\begin{equation}
\begin{array}{rcl}
I(a,b) &=&\displaystyle  
\sum_{\beta=\alpha(a)
           -t(a)}^{\alpha(a)-t(a)+k}g(x_{\beta})\int_a^{x_{\alpha(a)+1}}dy\,w_{\beta,t}^{(k)}(y)\\
\\
&+&\displaystyle
    \sum_{\gamma=\alpha(a)+1}^{\alpha(b)}\sum_{\beta=\gamma-t_\gamma}^{\gamma-t_\gamma+k}g(x_{\beta})\int_{x_\gamma}^{x_{\gamma+1}}dy\,w_{\beta,t}^{(k)}(y)\\
\\
&-&\displaystyle \sum_{\beta=\alpha(b)-t(b)}^{\alpha(b)
    -t(b)+k}g(x_{\beta})\int_b^{x_{\alpha(b)+1}}dy\,w_{\beta,t}^{(k)}(y)\,,
\end{array}
\label{eq:integralT}
\end{equation}
with:
\begin{equation}
t_\gamma = t(x_\gamma) = \mbox{max}\left[\mbox{min}\left[\gamma,\alpha_t -2\right] -\alpha_t+k + 1, 0\right] \,.
\end{equation}
It turns out to be very complicated to write the expression in
Eq.~(\ref{eq:integralT}) in a more compact form, such as that in
Eq.~(\ref{eq:integralT0}). Therefore, we implement
Eq.~(\ref{eq:integralT}) as it is. The one thing left to do is to
solve the integrals. This is done by using the general formula:
\begin{equation}
\begin{array}{rcl}
  \displaystyle\int_{\ell_1}^{\ell_2}dy\,w_{\beta,t}^{(k)}(y)
  &=&\displaystyle \sum_{i=0}^{{\rm min}(k,\beta)}
                                                                      \theta(\ell_2-x_{\beta-i+t_\ell})\theta(x_{\beta-i+t_\ell+1}-\ell_1)\\
  \\
                                                                  &\times&\displaystyle \left[\prod^{k}_{m=0\atop m\ne
                                                                           i}\frac{1}{x_{\beta}-x_{\beta-i+m}}\right]
                                                                           \sum_{n=0}^k \frac{(-1)^n p_{\beta}^{(k)}(n)}{k-n+1} \left(\overline{\ell}_2^{k-n+1}-\overline{\ell}_1^{k-n+1}\right)\,,
\end{array}
\end{equation}
with $t(\ell_1)=t(\ell_2)=t_\ell$, and
$\overline{\ell}_1={\rm max}(\ell_1,x_{\beta-i+t_\ell})$ and
$\overline{\ell}_2={\rm min}(\ell_2,x_{\beta-i+t_\ell+1})$.

\end{document}
