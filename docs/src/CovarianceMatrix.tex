\documentclass[10pt,a4paper]{article}
\usepackage{amsmath,amssymb,bm,graphicx,pictex,makeidx,subfigure}
\usepackage[italian,english]{babel}
\usepackage[center,small]{caption}[2007/01/07]
\usepackage{fancyhdr}
\usepackage{color}

\definecolor{blu}{rgb}{0,0,1}
\definecolor{verde}{rgb}{0,1,0}
\definecolor{rosso}{rgb}{1,0,0}

\begin{document}

\section{The $\chi^2$ in the presence of correlations}

Suppose to have an ensamble of $n$ measurements having the following
structure:
\begin{equation}
  m_i\pm \sigma_{i,\rm stat} \pm \sigma_{i,\rm unc} \pm \sigma_{i,\rm
    corr}^{(1)}\pm\dots \pm \sigma_{i,\rm
    corr}^{(k)}\,,
\end{equation}
where $m_i$, with $i=1,\dots, n$, is the central value of the $i$-th
measurement, $\sigma_{i,\rm stat}$ its (uncorrelated) statistical
uncertainty, $\sigma_{i,\rm unc}$ its uncorrelated systematic
uncertainty\footnote{There could be more than one uncorrelated
  systematic uncertainty. In this case, $\sigma_{i,\rm unc}$ is just
  the square root of the sum in quadrature of all the uncorrelated
  systematic uncertainties.}, and $\sigma_{i,\rm corr}^{(l)}$, with
$l=1,\dots,k$, its correlated systematic uncertainties. With this
information at hand, one can construct the full covariance matrix
$V_{ij}$ as follows (see for example Ref.~\cite{Ball:2012wy}):
\begin{equation}\label{eq:covmat}
  V_{ij}=\left(\sigma_{i,\rm stat}^2 +\sigma_{i,\rm unc}^2\right)\delta_{ij} + \sum_{l=1}^{k}\sigma_{i,\rm
    corr}^{(l)}\sigma_{j,\rm
    corr}^{(l)}\,.
\end{equation}
This is a clearly symmetric matrix. Given a set of predictions $t_i$
corresponding to the $n$ measurements of the ensamble, the $\chi^2$
takes the form:
\begin{equation}\label{eq:chi2cov}
  \chi^2=
  \sum_{i,j=1}^{n}\left(m_i-t_i\right)V_{ij}^{-1}\left(m_j-t_j\right) =
  \mathbf{y}^{T} \cdot \mathbf{V}^{-1} \cdot \mathbf{y}\,,
\end{equation}
where in the second equality we have used the matricial notation and
defined $y_i = m_i-t_i$. A convenient way to compute the $\chi^2$
relies on the Cholesky decomposition of the covariance matrix
$\mathbf{V}$. In particular, it can be proven that any symmetric and
positive definite matrix $\mathbf{V}$ can be decomposed as:
\begin{equation}\label{eq:choleskydec}
\mathbf{V} = \mathbf{L}\cdot\mathbf{L}^{T}\,,
\end{equation}
where $\mathbf{L}$ is a lower triangular matrix whose entries are
related recursively to those of $\mathbf{V}$ as follows:
\begin{equation}\label{eq:cholalg}
\begin{array}{rcl}
  L_{kk} &=&\displaystyle \sqrt{V_{kk}-\sum_{j=1}^{k-1}L_{kj}^2}\,,\\
  \\
  L_{ik} &=&\displaystyle
             \frac{1}{L_{kk}}\left(V_{ik}-\sum_{j=1}^{k-1}L_{ij}L_{kj}\right)\,,\quad
             k < i\,,\\
\\
  L_{ik} &=&\displaystyle 0\,,\quad
             k > i\,.\\
\end{array}
\end{equation}
It is then easy to see that the $\chi^2$ can be written as:
\begin{equation}
\chi^2 = \left|\mathbf{L}^{-1}\cdot \mathbf{y}\right|^2\,.
\end{equation}
But the vector $\mathbf{x} \equiv \mathbf{L}^{-1}\cdot \mathbf{y}$ is
the solution of the linear system:
\begin{equation}
  \mathbf{L} \cdot \mathbf{x} = \mathbf{y}\,,
\end{equation}
that can be efficiently solved by forward substitution, so that:
\begin{equation}
  \chi^2 = \left|\mathbf{x}\right|^2\,.
\end{equation}
Following this procedure, one does not need to compute explicitly the
inverse of the covariance matrix $\mathbf{V}$, simplifying
significantly the computation of the $\chi^2$.

\section{Additive and multiplicative uncertainties}

The correlated systematic uncertainties $\sigma_{i,\rm corr}^{(l)}$
may be either \textit{additive} or \textit{multiplicative}. The nature
of the single uncertainties is typically provided by the experiments
that release the measurements. A typical example of multiplicative
uncertainty is the luminosity uncertainty but there can be others.

Now let us express all the correlated systematic uncertainties
$\sigma_{i,\rm corr}^{(l)}$ as relative to the associate central value
$m_i$, so that we define\footnote{Note that this redefinition does not
  change the nature of the uncertainties, additive uncertainties
  remain additive as well as multiplicative uncertainties remain
  multiplicative.}:
\begin{equation}
\sigma_{i,\rm corr}^{(l)}\equiv  \delta_{i,\rm corr}^{(l)} m_i
\end{equation}
and let us also define
$s_i^2\equiv \sigma_{i,\rm stat}^2 +\sigma_{i,\rm unc}^2$ so that
Eq.~(\ref{eq:covmat}) can be rewritten as:
\begin{equation}\label{eq:covmat2}
  V_{ij}=s_i^2\delta_{ij} + \left(\sum_{l=1}^{k}\delta_{i,\rm
    corr}^{(l)}\delta_{j,\rm
    corr}^{(l)}\right)m_im_j\,.
\end{equation}
Now we split the correlated systematic uncertainties into $k_a$
additive uncertainties and $k_m$ multiplicative uncertainties, such
that $k_a+k_m=k$. This way Eq.~(\ref{eq:covmat2}) takes the form:
\begin{equation}\label{eq:covmat3}
  V_{ij}=s_i^2\delta_{ij} + \left(\sum_{l=1}^{k_a}\delta_{i,\rm
    add}^{(l)}\delta_{j,\rm
    add}^{(l)}+\sum_{l=1}^{k_m}\delta_{i,\rm
    mult}^{(l)}\delta_{j,\rm
    mult}^{(l)}\right)m_im_j\,.
\end{equation}
It is believed that this definition of the covariance matrix is
problematic in that it results in the so-called D'Agostini bias of the
multiplicative uncertainties~\cite{DAgostini:1993arp}. A possible
solution to this problem is the so-called
$t_0$-prescription~\cite{Ball:2009qv}, where the experimental central
value $m_i$ in the multiplicative term is replaced by a fixed
theoretical predictions $t_i^{(0)}$, typically computed in a previous
fit in which the ``standard'' definition of the covariance matrix in
Eq.~(\ref{eq:covmat}) (often referred to as \textit{experimental}
definition) is used. Applying the $t_0$ prescription, the covariance
matrix takes the form:
\begin{equation}\label{eq:covmat4}
  V_{ij}=s_i^2\delta_{ij} + \sum_{l=1}^{k_a}\delta_{i,\rm
    add}^{(l)}\delta_{j,\rm
    add}^{(l)}m_im_j+\sum_{l=1}^{k_m}\delta_{i,\rm
    mult}^{(l)}\delta_{j,\rm
    mult}^{(l)}t_i^{(0)}t_j^{(0)}\,.
\end{equation}

\section{Artificial generation of correlated systematics}

In order to implement the definition of the $\chi^2$ discussed above,
it is necessary to have the experimental information in terms of the
correlated systematic uncertainties $\sigma_{i,\rm corr}^{(l)}$. This
is what the experimental collaborations usually release. However, in
some cases this information is given in terms of a covariance
matrix. Therefore, one needs to find a workaround to generate
correlated systematic uncertainties out of a covariance matrix. 

Given a $n \times n$ symmetric matrix $\mathbf{C}$, it will have $n$
orthonormal eigenvectors $\mathbf{x}^{(i)}$, such that
$\mathbf{x}^{(i)}\cdot \mathbf{x}^{(j)}=\delta_{ij}$, each of which
will have a non-negative eigenvalue $\lambda_i$ associated:
\begin{equation}
  \mathbf{C}\cdot \mathbf{x}^{(i)} = \lambda_i \mathbf{x}^{(i)}\,, \quad
  i =1,\dots,n\,.
\end{equation}
If we define:
\begin{equation}\label{eq:artsys}
  \sigma_{i,\rm corr}^{(l)} = \sqrt{\lambda_l} x_i^{(l)}\,,\quad i,l=1,\dots,n\,,
\end{equation}
one can show that:
\begin{equation}\label{eq:artsydef}
  \sum_{l=1}^{n}\sigma_{i,\rm corr}^{(l)}\sigma_{j,\rm corr}^{(l)} =C_{ij}\,.
\end{equation}
To prove this equality we start from the following matricial relation:
\begin{equation}
\mathbf{C} =\mathbf{Q}\cdot \mathbf{\Lambda}\cdot \mathbf{Q}^{-1}\,,
\end{equation}
where $\mathbf{\Lambda}$ is a diagonal matrix with the eigenvalues
$\lambda_i$ on the diagonal ($\Lambda_{ij} = \lambda_i\delta_{ij}$),
while $\mathbf{Q}$ is a matrix whose columns are the eigenvectors
$\mathbf{x}^{(i)}$ ($Q_{ij} = x_{i}^{(j)}$). In addition, since in
this particular case
$\mathbf{x}^{(i)}\cdot \mathbf{x}^{(j)}=\delta_{ij}$, this implies
that:
\begin{equation}
\mathbf{Q}^T \cdot \mathbf{Q}  = \mathbf{I}\quad\Rightarrow\quad
\mathbf{Q}^{-1} =  \mathbf{Q}^{T}\,,
\end{equation}
so that:
\begin{equation}\label{eq:diagC}
  \mathbf{C} =\mathbf{Q}\cdot \mathbf{\Lambda}\cdot \mathbf{Q}^{T}\,.
\end{equation}
It follows that:
\begin{equation}
  C_{ij} =\sum_{k,l=1}^{n}Q_{ik}\Lambda_{kl} Q_{jl} = \sum_{k,l=1}^{n} x_{i}^{(k)}
  \lambda_{k}\delta_{kl}x_{j}^{(l)} = \sum_{l=1}^{n} \lambda_{l}x_{i}^{(l)}
  x_{j}^{(l)}=\sum_{l=1}^{n}\sigma_{i,\rm corr}^{(l)}\sigma_{j,\rm corr}^{(l)} \,,
\end{equation}
as required.

The matrix \textbf{C} can be regarded as the correlated contribution to
the full covariance matrix \textbf{V}. In particular, considering
Eqs.~(\ref{eq:covmat}) and~(\ref{eq:covmat2}), one can write:
\begin{equation}
\mathbf{V} = \mathbf{U} + \mathbf{C}\,,
\end{equation}
where $\mathbf{U}$ is a diagonal matrix of uncorrelated uncertainties:
\begin{equation}
U_{ij} = s_i^2\delta_{ij}\,.
\end{equation}
This defines the matrix $\mathbf{C}$ as:
\begin{equation}\label{eq:corrdef}
\mathbf{C} = \mathbf{V} - \mathbf{U}\,,
\end{equation}
such that, given a $n\times n$ covariance matrix $\mathbf{V}$ along
with its uncorrelated contribution $\mathbf{U}$, one can generate a
set of $n$ \textit{artificial} correlated systematics according to
Eq.~(\ref{eq:artsys}), where $\mathbf{C}$ is given in
Eq.~(\ref{eq:corrdef}), for each of the $n$ measurements. This allows
us to implement Eq.~(\ref{eq:covmat4}) for the construction of the
covariance matrix.

\section{Determining the systematic shifts}

In order to visualise the effect of systematic uncertainties, it is
instructive to compute the \textit{systematic shift} generated by the
systematic uncertainties. To do so, we need to write the $\chi^2$ in
terms of the so-called ``nuisance parameters'' $\lambda_\alpha$. One
can show that the definition of the $\chi^2$ in Eq.~(\ref{eq:chi2cov})
is equivalent to~\cite{Ball:2012wy}:
\begin{equation}\label{eq:chi2nuis}
\chi^2 = \sum_{i=1}^n\frac{1}{s_i^2}\left(m_i -t_i
  -\sum_{\alpha=1}^k\lambda_\alpha \sigma_{i,\rm corr}^{(\alpha)} \right)^2 + \sum_{\alpha=1}^k\lambda_\alpha^2\,.
\end{equation}
The optimal value of the nuisance parameters can be computed by
minimising the $\chi^2$ with respect to them, that is imposing:
\begin{equation}
\frac{\partial \chi^2}{\partial \lambda_\beta} = 0\,.
\end{equation}
This yields the system:
\begin{equation}\label{eq:nuissys}
  \sum_{\beta=1}^kA_{\alpha\beta}\lambda_\beta =\rho_\alpha\,,
\end{equation}
with:
\begin{equation}\label{eq:sysing}
A_{\alpha\beta}= \delta_{\alpha\beta}+\sum_{i=1}^n\frac{\sigma_{i,\rm corr}^{(\alpha)}\sigma_{i,\rm corr}^{(\beta)}}{s_i^2}\quad\mbox{and}\quad \rho_\alpha=\sum_{i=1}^n\frac{m_i-t_i}{s_i^2}\sigma_{i,\rm corr}^{(\alpha)}\,,
\end{equation}
that determines the values of $\lambda_\beta$. The quantity:
\begin{equation}
d_i =\sum_{\alpha=1}^k\lambda_\alpha \sigma_{i,\rm corr}^{(\alpha)}
\end{equation}
in Eq.~(\ref{eq:chi2nuis}) can be interpreted as a shift caused by the
correlated systematic uncertainties. Defining the shifted predictions
as:
\begin{equation}
\overline{t}_i =t_i+d_i\,,
\end{equation}
the $\chi^2$ reads:
\begin{equation}\label{eq:chi2nuisshift}
  \chi^2 = \sum_{i=1}^n\left(\frac{m_i -\overline{t}_i}{s_i}\right)^2 + \sum_{\alpha=1}^k\lambda_\alpha^2\,.
\end{equation}
Therefore, up to a penalty term given by the sum of the square of the
nuisance parameters, the $\chi^2$ takes the form of the uncorrelated
definition. In order to achieve a visual assessment of the agreement
between data and theory, it appears natural to compare the central
experimental values $m_i$ to the shifted theoretical predictions
$\overline{t}_i$ in units of the uncorrelated uncertainty $s_i$.

\section{Effect of cuts on the $\chi^2$}

A relevant question to ask is what is the effect of possible cuts on
the input dataset on the form of the $\chi^2$ in the presence of
correlations. In order to address this question, we consider a simple
dataset made of two datapoints equipped with a single uncorrelated
uncertainty and a single fully-correlated uncertainty:\footnote{Note
  that we need to include \textit{also} an uncorrelated uncertainty
  because otherwise the covariance matrix would be singular. This a
  symptom of the fact that if no uncorrelated uncertainties are
  present, the two points would be totally dependent on each
  other. This observation will be relevant in the following.}
\begin{equation}\label{eq:fullset}
S:\quad\{m_i\pm \sigma_i^u\pm \sigma_i^c\}\,,\quad i  =1,2\,.
\end{equation}
The resulting covariance matrix reads:
\begin{equation}
\mathbf{V}=\begin{pmatrix}
(\sigma_1^u)^2+ (\sigma_1^c)^2 & \sigma_1^c \sigma_2^c \\
\sigma_1^c \sigma_2^c & (\sigma_2^u)^2+ (\sigma_2^c)^2
\end{pmatrix}\,,
\end{equation}
with inverse:
\begin{equation}
\mathbf{V}^{-1}=\frac{1}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\begin{pmatrix}
(\sigma_2^u)^2+ (\sigma_2^c)^2 & -\sigma_1^c \sigma_2^c \\
-\sigma_1^c \sigma_2^c & (\sigma_1^u)^2+ (\sigma_1^c)^2
\end{pmatrix}\,.
\end{equation}
In addition, we a have a set of two theoretical predictions $\{t_i\}$
that allows us to define a column vector of residuals
$\mathbf{r}^{\rm T}=(r_1,\;r_2)$ with $r_i=m_i-t_i$. The $\chi^2$ then takes
the following explicit expression:
\begin{equation}\label{eq:fullchi2}
  \chi^2= \mathbf{r}^{\rm T}\cdot \mathbf{V}^{-1}\cdot \mathbf{r}=
  \frac{r_1^2\left[(\sigma_2^u)^2+ (\sigma_2^c)^2\right]+r_2^2\left[(\sigma_1^u)^2+ (\sigma_1^c)^2\right]-2 r_1 r_2\sigma_1^c \sigma_2^c}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\,.
\end{equation}
Now, suppose we want to exclude the datapoint with $i=2$ from the
definition of the $\chi^2$. The possibly most natural way to proceed
is to exclude the point, along with its uncertainties, directly from
the set in Eq.~(\ref{eq:fullset}). This straightforwardly leads to the
following $\chi^2$:
\begin{equation}\label{eq:redchi21}
{\chi}_{\rm cut\;1}^2 = \frac{r_1^2}{(\sigma_1^u)^2+ (\sigma_1^c)^2}\,.
\end{equation}
However, this procedure might be questioned in that it artificially
modifies the original dataset by effectively defining a \textit{new}
dataset where the point $i=2$ is not present. In the absence of
correlations this is a sound procedure because each single datapoint
can be regarder an independent subset of the original
set. However, this is not the case when correlations are present.

Therefore, it is necessary to devise a method that avoids any
modification of the dataset but yet allows one to prevent specific
datapoints to contribute to the $\chi^2$. In general, the purpose of
excluding datapoints from a fit is to avoid that the model used to
compute the predictions is forced outside its application region. One
can therefore \textit{enforce} that the model exactly agrees with the
datapoints to be excluded. This effectively amounts to set $t_2=m_2$,
or equivalently $r_2=0$, reducing Eq.~(\ref{eq:fullchi2}) to:
\begin{equation}\label{eq:fullchi2trunc}
  \chi_{\rm cut\;2}^2= 
  \frac{r_1^2\left[(\sigma_2^u)^2+ (\sigma_2^c)^2\right]}{(\sigma_1^u \sigma_2^u)^2+ (\sigma_1^c \sigma_2^u)^2+ (\sigma_1^u \sigma_2^c)^2}\,.
\end{equation}
This has the clear advantage that the experimental information,
including correlations, remains totally intact. In order to see that
Eq.~(\ref{eq:fullchi2trunc}) has the right properties, we first take
the limit for $\sigma_2^c\rightarrow 0$. This gives:
\begin{equation}
  \lim_{\sigma_2^c\rightarrow 0}\chi_{\rm cut}^2= 
  \frac{r_1^2}{(\sigma_1^u)^2+ (\sigma_1^c)^2}\,,
\end{equation}
which reproduces the result of Eq.~(\ref{eq:redchi21}). This was to be
expected because no reference to the point $i=2$ can remain after the
removal of its correlation to the point $i=1$. In addition, it is also
very instructive to take the limit $\sigma_2^u\rightarrow 0$
Eq.~(\ref{eq:fullchi2trunc}), which gives:
\begin{equation}
\lim_{\sigma_2^u\rightarrow 0}\chi_{\rm cut}^2= 
\frac{r_1^2}{ (\sigma_1^u)^2}\,.
\end{equation}
In this case, not only any reference to the point $i=2$ drops out, but
also the dependence of the $\chi^2$ on the correlation uncertainty of
the point $i=1$ disappears. Despite at first sight this looks
counterintuitive, it seems to be the correct behaviour. To see this,
we observe that, if the point $i=2$ has no uncorrelated uncertainty,
its fluctuations are totally driven by the fluctuations of the point
$i=1$. Therefore, the point $i=2$ is completely dependent on the point
$i=1$ and thus cannot give any contribution to the $\chi^2$. In
addition, the correlation uncertainty of the point $i=1$ must also be
irrelevant because, being $i=2$ totally correlated to $i=1$, any
correlation of $i=1$ to $i=2$ translates into a correlation to itself
and this cannot contribute to the $\chi^2$ either. Importantly, if we
take the limit $\sigma_1^u\rightarrow 0$, the $\chi^2$ diverges no
matter the value of $\sigma_1^c$. This is consistent with setting
$\sigma_1^u=\sigma_2^u$ since the beginning in Eq.~(\ref{eq:fullset})
which indeed gives a singular $\chi^2$ because the covariance matrix
is singular. Eq.~(\ref{eq:redchi21}) does not produce these features
hanging in favour of Eq.~(\ref{eq:fullchi2trunc}).

\section{Monte Carlo replica generation}

In this section we consider the Monte Carlo generation of articial
replicas. The aim is to obtain a formula for the generation of
replicas that gives rise to a figure of merit distributed correctly,
\textit{i.e.} that follows a $\chi^2$ distribution with as many
degrees of freedom as number of data points. Following
Ref.~\cite{wiki:xxx}, we require that the $\chi^2$, that in matricial
form can be expressed as:
\begin{equation}\label{eq:chi2matrix}
  \chi^2 = \mathbf{y}^{T}\cdot \mathbf{V}^{-1} \cdot \mathbf{y}\,,
\end{equation}
where $\mathbf{V}$ is the $n\times n$ covariance matrix, being $n$ the
number of datapoints, and $\mathbf{y}$ is a column vector with $n$
entries that can be expressed as:
\begin{equation}
y_j = f_j-m_j\,,
\end{equation}
with $m_j$ a given value (to be thought as the central value) and
$f_j$ its fluctuated value, is distributed like:
\begin{equation}\label{eq:wikipedia}
\chi^2\sim \sum_{i=1}^{n}z_i^2=|\mathbf{z}|^2\,,
\end{equation}
where $z_1,\dots , z_n$ are independent, standard normal random
variables, \textit{i.e.} such that:
\begin{equation}\label{eq:averages}
\langle z_i\rangle=0\quad\mbox{and}\quad \langle z_iz_j\rangle = \delta_{ij}\,,
\end{equation}
where the symbol $\langle\,\dots\rangle$ indicates the
average.\footnote{The relevant indentities are:
\begin{equation}
  \langle z_i^{2n-1}\rangle = 0\quad\mbox{and}\quad \langle
  z_i^{2n}\rangle= (2n-1)!!\quad n=1,2,3,\dots\,,
\end{equation}
where $(2n-1)!!=1\cdot3\cdot\dots\cdot(2n-1)$ is the odd
semi-factorial. When cosidering combinations of two random variables
the result is:
\begin{equation}
  \langle z_i^n z_j^m\rangle = \left\{
\begin{array}{ll}
\langle z_i^{n+m}\rangle&\quad i = j\\
\langle z_i^n\rangle\langle z_j^m\rangle&\quad i \neq j
\end{array}
\right.
\end{equation}} This immediatelly tells us that:
\begin{equation}
\langle\chi^2\rangle=n\,.
\end{equation}
As already discussed above, Eq.~(\ref{eq:chi2matrix}) can
alternatively be written as:
\begin{equation}\label{explicitchi2chol}
\chi^2 = \left|\mathbf{L}^{-1}\cdot \mathbf{y}\right|^2\,,
\end{equation}
where $\mathbf{L}$ is the Cholesky decomposition of the matrix
$\mathbf{V}$, such that:
\begin{equation}\label{eq:choleskydecagain}
\mathbf{V} = \mathbf{L}\cdot\mathbf{L}^{T}\,.
\end{equation}
Comparing Eqs.~(\ref{explicitchi2chol}) and~(\ref{eq:wikipedia}), one
can infer that the following relation should hold:
\begin{equation}
\mathbf{L}^{-1}\cdot \mathbf{y}\sim\mathbf{z}\,,
\end{equation}
which immediately implies:
\begin{equation}
\mathbf{y}\sim\mathbf{L}\cdot\mathbf{z}\,,
\end{equation}
or equivalently:
\begin{equation}\label{eq:deviations}
f_j\sim m_j+\sum_{i=1}^nL_{ji}z_i\,.
\end{equation}
In conclusion, in order for the $\chi^2$ to have the correct
distribution, the fluctuations $f_j$ around the experimental central
values $m_j$ of a given Monte Carlo replica have to be computed
according to Eq.~(\ref{eq:deviations}).

We now show that Eq.~(\ref{eq:deviations}) reproduces the statistical
properties of the original ensamble of data. To do so, we compute
average and correlations of Eq.~(\ref{eq:deviations}) using
Eq.~(\ref{eq:averages}). The average gives:
\begin{equation}
\langle f_j\rangle = m_j+\sum_{i=1}^nL_{ji}\langle z_i \rangle = m_j\,,
\end{equation}
as expected. The correlation instead gives:
\begin{equation}
\begin{array}{rcl}
\langle f_j f_k\rangle &=& \displaystyle \left\langle
  \left(m_j+\sum_{i=1}^nL_{ji}z_i\right)
  \left(m_k+\sum_{i'=1}^nL_{ki'}z_{i'}\right)\right\rangle\\
\\
&=& \displaystyle m_jm_k+\sum_{i,i'=1}^nL_{ji'}L_{ki'}\langle z_{i'}
    z_{i'}\rangle=\langle f_j \rangle\langle f_k\rangle+V_{jk}\,,
\end{array}
\end{equation}
again as expected. These are fundamental requirements of a sound Monte
Carlo ensamble (see also Eq.~(10) of Ref.~\cite{Ball:2009qv}).

Eq.~(\ref{eq:deviations}) should be compared to the more ``standard''
Monte-Carlo replica generation formulas such as those given in
Refs.~\cite{Ball:2008by, Ball:2014uwa}. We now specifically consider
Eq.~(20) of Ref.~\cite{Ball:2014uwa} that we report here in terms of
relative uncertainties $\delta$'s matching our notation:
\begin{equation}\label{eq:nnpdfmc}
\widetilde{f}_j\sim m_j\prod_{m=1}^{k_m}\left(1+\delta_{{\rm mult},j}^{(m)} x_m\right)\left(1+\delta_{{\rm unc},j} z_j+\sum_{a=1}^{k_a} \delta_{{\rm add},j}^{(a)} y_a\right)\,,
\end{equation}
where $z_j$, $y_a$, and $x_m$ are independent random variables. We now
expand the product in the front up to second order in $x$
\begin{equation}
  \prod_{m=1}^{k_m}\left(1+\delta_{{\rm mult},j}^{(m)}
    x_m\right)=1+\sum_{m=1}^{k_m}\delta_{{\rm mult},j}^{(m)}
  x_m+\sum_{m=1}^{k_m}\sum_{m'=1\atop m'\neq m}^{k_m}\delta_{{\rm
      mult},j}^{(m)}\delta_{{\rm mult},j}^{(m')}x_m x_{m'}+\mbox{trilinear}\,,
\end{equation}
where the ``trilinear'' corrections correspond to combinations of the
kind $x_ix_jx_k$, with $i\neq j\neq k\neq i$, such that their average
vanishes:
$\langle x_ix_jx_k\rangle = \langle x_i\rangle \langle x_j
\rangle\langle x_k\rangle = 0$. This allows us to expand
Eq.~(\ref{eq:nnpdfmc}) as:
\begin{equation}\label{eq:nnpdfmcexp}
\begin{array}{rcl}
\widetilde{f}_j&\sim&\displaystyle m_j\Bigg(1+\delta_{{\rm unc},j} z_j+\sum_{a=1}^{k_a} \delta_{{\rm add},j}^{(a)} y_a +\sum_{m=1}^{k_m}\delta_{{\rm mult},j}^{(m)}
  x_m\\
\\
&+&\displaystyle\delta_{{\rm unc},j} \sum_{m=1}^{k_m} \delta_{{\rm mult},j}^{(m)}
    z_jx_m+\sum_{a=1}^{k_a} \sum_{m=1}^{k_m}\delta_{{\rm add},j}^{(a)} \delta_{{\rm mult},j}^{(m)} y_a 
  x_m\\
\\
&+&\displaystyle \sum_{m=1}^{k_m}\sum_{m'=1\atop m'\neq m}^{k_m}\delta_{{\rm
      mult},j}^{(m)}\delta_{{\rm mult},j}^{(m')}x_m x_{m'}\Bigg)+\mbox{trilinear}\,.
\end{array}
\end{equation}
Since Eq.~(\ref{eq:nnpdfmcexp}) involves only terms that have at most
one power of the same random variable, the average of
$\widetilde{f}_j$ automatically yields:
\begin{equation}\label{eq:centralvaluenn}
  \langle \widetilde {f}_j\rangle = m_j\,,
\end{equation}
in agreement with the expectation. We now compute the correlations
making use of the combination rules of random variables. This gives:
\begin{equation}\label{eq:corrnn}
\begin{array}{rcl}
  \langle \widetilde{f}_j \widetilde{f}_k\rangle&\sim&\displaystyle m_jm_k\Bigg(1+\delta_{{\rm unc},j}^2\delta_{jk}+\sum_{a=1}^{k_a} \delta_{{\rm add},j}^{(a)}\delta_{{\rm add},k}^{(a)} +\sum_{m=1}^{k_m}\delta_{{\rm mult},j}^{(m)}\delta_{{\rm mult},k}^{(m)}\\
\\
&+&\displaystyle\delta_{{\rm unc},j}^2 \sum_{m=1}^{k_m} \delta_{{\rm mult},j}^{(m)}\delta_{{\rm  mult},k}^{(m)}
  +\sum_{a=1}^{k_a} \sum_{m=1}^{k_m}\delta_{{\rm add},j}^{(a)}\delta_{{\rm add},k}^{(a)} \delta_{{\rm mult},j}^{(m)} \delta_{{\rm mult},k}^{(m)} \\
\\
&+&\displaystyle \sum_{m=1}^{k_m}\sum_{m'=1\atop m'\neq m}^{k_m}\delta_{{\rm
      mult},j}^{(m)}\delta_{{\rm
      mult},k}^{(m)}\delta_{{\rm mult},j}^{(m')}\delta_{{\rm
    mult},k}^{(m')}+\mathcal{O}(\delta^6)\Bigg)\\
\\
&=&\displaystyle \langle \widetilde{f}_j\rangle \langle
    \widetilde{f}_k\rangle +V_{jk}+\mathcal{O}(\sigma_{\rm mult}^2 s^2, \sigma_{\rm mult}^2 \sigma_{\rm add}^2)\,.
\end{array}
\end{equation}
In conclusion, Eq.~(\ref{eq:nnpdfmc}) reproduces exactly the central
values of the original ensemble (see Eq.~(\ref{eq:centralvaluenn}))
while it reproduces correlations only up to corrections proportional
to the fourth power of the typical uncertainty. Interestingly, these
corrections are all proportional to $\sigma_{\rm mult}^2$. This means
that, in the absence of multiplicative uncertainties, somewhat
expectedly Eq.~(\ref{eq:nnpdfmc}) \textit{exactly} reproduces the
correlations. On the other hand, if multiplicative uncertainties are
large these correction become potentially sizeable preventing
Eq.~(\ref{eq:nnpdfmc}) from correctly reproducing the correlations of
the original ensemble.

The case of Eq.~(13) of Ref.~\cite{Ball:2008by} is more involved
because of the presence of the square-root sign for the relative
multiplicative uncertainties. This case can be handled again be
expanding to second order the factor depending on multiplicative
uncertainties with the only difference that, due to the expansion:
\begin{equation}
  \sqrt{1+x}=1+\frac12x-\frac18x^2+\mathcal{O}(x^3)\,,
\end{equation}
the square root produces a factor $1/2$ in front of the linear terms
and a factor $-1/8$ in front of the quadratic terms. This is expected
to reduce the impact of the corrections terms in
Eq.~(\ref{eq:corrnn}).

It is instructive to work out whether Eq.~(\ref{eq:deviations}) can be
directly related to the Monte-Carlo replicas generation formulas given
in Refs.~\cite{Ball:2008by, Ball:2014uwa}. The aim is to find a
closed-form expression for the Cholesky-decomposition matrix
$\mathbf{L}$ of the covariance matrix $\mathbf{V}$ in terms of the
single uncertainties. Through Eq.~(\ref{eq:deviations}), this will
finally allow us to derive a formula of the same kind of those given
in Refs.~\cite{Ball:2008by, Ball:2014uwa}. The basic observation is
that the covariance matrix can be written as:
\begin{equation}
\mathbf{V} = \mathbf{D}+ \sum_{l=1}^k 
\mathbf{V}^{(l)} = \sum_{l=0}^k \mathbf{L}^{(l)}\cdot \mathbf{L}^{(l)T}\,,
\end{equation}
with:
\begin{equation}
L_{ij}^{(l)}=\left\{
\begin{array}{ll}
s_i\delta_{ij}&\quad l = 0\,,\\
\sigma_i^{(l)}\delta_{j1}&\quad 1\leq l\leq k\,.
\end{array}
\right.
\end{equation}
On the other hand, we also know that:
\begin{equation}
  \mathbf{V} = \mathbf{L}\cdot \mathbf{L}^T\,,
\end{equation}
therefore:
\begin{equation}
  \mathbf{L}\cdot \mathbf{L}^T=\sum_{l=0}^k \mathbf{L}^{(l)}\cdot \mathbf{L}^{(l)T}\,.
\end{equation}
The question is to express $\mathbf{L}$ in terms of all the
$\mathbf{L}^{(l)}$. Unfortunately though this equation has no general
solution. However, a step forward can be done be exploiting the fact
that the matrices $\mathbf{V}^{(l)}$ are positive semidefinite
(\textit{i.e.} they have zero determinant) and thus their Cholesky
decomposition is not unique. In particular, provided that $k\leq n$,
one can choose:
\begin{equation}\label{eq:choldecomp2}
\widetilde{L}_{ij}^{(l)}=
\sigma_i^{(l)}\delta_{jl}\,,\quad 1\leq l\leq k\leq n\,,
\end{equation}
such that:
\begin{equation}
  \left(\sum_{l=1}^k\widetilde{\mathbf{L}}^{(l)}\right)\cdot\left(
    \sum_{l'=1}^k\widetilde{\mathbf{L}}^{(l')}\right)=\sum_{l=1}^k 
  \mathbf{V}^{(l)}\,.
\end{equation}
However, choosing:
\begin{equation}\label{eq:CholVapprox}
\widetilde{\mathbf{L}} = \mathbf{L}^{(0)} + \sum_{l=1}^k\widetilde{\mathbf{L}}^{(l)}\,,
\end{equation}
and multiplying it for its transpose would give:
\begin{equation}
\widetilde{\mathbf{L}}\cdot \widetilde{\mathbf{L}}^T = \mathbf{V} + \sum_{l=1}^k\left(\mathbf{L}^{(0)}\cdot \widetilde{\mathbf{L}}^{(l)T}+\widetilde{\mathbf{L}}^{(l)}\cdot \mathbf{L}^{(0)T}\right)\,,
\end{equation}
that gives back the covariance matrix plus a spurious term. Writing
the expression above in terms of single entries, one finds:
\begin{equation}
\sum_{m=1}^n\widetilde{L}_{im}\widetilde{L}_{jm} = V_{ij} + s_i \sigma_{j}^{(i)}+s_j\sigma_{i}^{(j)}\,.
\end{equation}
The peculiarity here is that the upper index of $\sigma_{j}^{(i)}$ and
$\sigma_{i}^{(j)}$, since $k\leq n$ (and in fact usually $k\ll n$,
\textit{i.e.} the number of correlated uncertainties is typically much
smaller than the number of data points), exceeds the allowed range. To
fix this problem we just assume $\sigma_{j}^{(i)}=0$ for $i>k$. This
finally gives:
\begin{equation}
\sum_{m=1}^n\widetilde{L}_{im}\widetilde{L}_{jm} = V_{ij}\,,\quad
\mbox{for } i,j>k\,,
\end{equation}
that means that, by adopting the Cholesky decomposition in
Eq.~(\ref{eq:choldecomp2}), part of the covariance matrix (usually the
largest part) is exactly recovered. Therefore,
$\widetilde{\mathbf{L}}$ Eq.~(\ref{eq:CholVapprox}) seems to be a
generally good approximator to the real Cholesky decomposition of the
covariance matrix $\mathbf{V}$. Plugging this equation into
Eq.~(\ref{eq:deviations}) immediately gives:
\begin{equation}\label{eq:deviations2}
  f_j\sim m_j+z_js_j + \sum_{l=1}^{k}z_l \sigma_{j,\rm
    corr}^{(l)}\,.
\end{equation}
It is interesting to focus on similarities and differences with the
formulas present in the literature. We will specifically consider
Eq.~(13) of Ref.~\cite{Ball:2008by} and Eq.~(20) of
Ref.~\cite{Ball:2014uwa}.

We start by considering the case in which there are only uncorrelated
and correlated additive uncertainties. In this case,
Refs.~\cite{Ball:2008by, Ball:2014uwa} are in mutual agreement and
only in partial agreement with our Eq.~(\ref{eq:deviations2}), the
difference being that the random numbers used to generate the
fluctuation associated to the $k$ correlated uncertainties are equal
to the first $k$ of the $n$ used for the uncorrelated uncertainties
(remember that we required $k\leq n$). Of course, if $k\ll n$, as it
often happens, the agreement between Eq.~(\ref{eq:deviations2}) and
Refs.~\cite{Ball:2008by, Ball:2014uwa} is asymptotically good.

When considering also correlated multiplicative uncertainties there
are differences across formulas of Refs.~\cite{Ball:2008by,
  Ball:2014uwa} and Eq.~(\ref{eq:deviations2}). While
Eq.~(\ref{eq:deviations2}) treats multiplicative uncertainties on the
same footing as additive uncertainties (consistently with the
construction of the covariance matrix in Eq.~(\ref{eq:covmat})),
Refs.~\cite{Ball:2008by, Ball:2014uwa} introduce a multiplicative
factor. In addition, Ref.~\cite{Ball:2008by} distinguishes between
relative and absolute multiplicative uncertainties.
\begin{figure}[h]
  \begin{centering}
    \includegraphics[width=0.49\textwidth]{plots/Chi2DistBABAR}
    \includegraphics[width=0.49\textwidth]{plots/Chi2DistALEPH}
    \caption{Distribution of the $\chi^2$'s of the Monte Carlo replica
      w.r.t. to the corresponding central values for BABAR (left) and
      ALEPH (right) experiments compared to the probability
      distribution function with to the appropriate number of degrees
      of freedom given by the number of data points. The blue
      histograms are produced using Eq.~(\ref{eq:deviations}), the red
      histogram is produced using Eq.~(13) of Ref.~\cite{Ball:2008by}
      where multiplicative uncertainties are treated as relative,
      while the green histogram is produced using Eq.~(20) of
      Ref.~\cite{Ball:2014uwa}. \label{fig:Chi2Dist}}
  \end{centering}
\end{figure}

\section{The D'Agostini ``bias''}

The main topic of this section is the so-called D'Agostini ``bias''
(DAB) originally presented in Ref.~\cite{DAgostini:1993arp}. Quoting
from the abstract of that paper: ``Best fits to data which are
affected by systematic uncertainties on the normalisation factor have
the tendency to produce curves lower than expected [...]. This bias
can become unacceptable if the normalisation error is large, or a
large number of data points are used''. In the following, I will
summarise the arguments behind this conclusion. I will then move to
giving my own view on this question arguing that the DAB is in fact
not a bias but rather a feature of correlation uncertainties in
general and not of multiplicative uncertainties in particular. This
implies that the distinction between additive and normalisation
uncertainties is effectively immaterial. Specifically, what really
triggers the DAB is \textit{not} the fact the uncertainty is on the
multiplication factor but rather that it typically gives raise to
correlated uncertainties that are different in \textit{absolute}
size. To show this, below I will show that the DAB takes place also in
the presence of additive uncertainties that are different in absolute
size. Conversely, we will show that the DAB does not takes place when
considering multiplicative uncertainties and the central values are
identical. The bottom line is that any difference in absolute size
between correlated uncertainties causes a deviation from the
expectation according to which the best fit value should be given by
the weighted average of the experimental central values weighted by
the inverse of the squared uncorrelated uncertainties. This deviation
from the expectation is the origin of the DAB.

In order to present the argument, Ref.~\cite{DAgostini:1993arp}
considers a data set made of two data points with the respective
uncorrelated uncertainty, $x_i\pm\sigma_i$ with $i=1,2$. In addition,
it is assumed that $x_i$ measure the same quantity and thus they can
be fitted with a single constant $\mu$. Two cases are treated. In the
first case the two data points are also affected by a single offset
(additive) correlated uncertainty $\sigma_c$. In the second case,
instead, the data points are affected by a normalisation correlated
uncertainty whose size is proportional to the central values through
the factor $\delta_f$. Given the simplicity of the problem, it is
possible in both cases to analytically compute the best value of $\mu$
and its uncertainty $\delta\mu$ by minimising the $\chi^2$ and
computing its second derivative at the minimum. As usual the $\chi^2$
is defined as:
\begin{equation}
\chi^2 = \sum_{i,j=1}^2y_iV^{-1}_{ij}y_j = \mathbf{y}^T\cdot
\mathbf{V}^{-1}\cdot \mathbf{y}\,,
\end{equation}
where $y_i=\mu-x_i$ and $V^{-1}_{ij}$ are the entries of the inverse
of the covariance matrix.

Let us start with the offset case in which the covariance matrix takes
the form:
\begin{equation}
\mathbf{V}=
\begin{pmatrix}
\sigma_1^2+\sigma_c^2& \sigma_c^2\\
\sigma_c^2& \sigma_2^2+\sigma_c^2
\end{pmatrix}\,,
\end{equation}
whose inverse is:
\begin{equation}
\mathbf{V}^{-1}=\frac{1}{D}
\begin{pmatrix}
\sigma_2^2+\sigma_c^2& -\sigma_c^2\\
-\sigma_c^2& \sigma_1^2+\sigma_c^2
\end{pmatrix}\,,
\end{equation}
with $D=\sigma_1^2 \sigma_2^2+(\sigma_1^2+\sigma_2^2)
\sigma_c^2$. With this we can straightforwardly compute the $\chi^2$:
\begin{equation}
\chi^2=\frac{\sigma_2^2y_1^2+\sigma_1^2y_2^2+(y_1-y_2)^2\sigma_c^2}{D}\,,
\end{equation}
that can be minimised w.r.t. to $\mu$ by requiring that:
\begin{equation}\label{eq:minchi2}
  \left.\frac{d\chi^2}{d\mu}\right|_{\mu=\overline{\mu}} = \left.2\frac{\sigma_2^2y_1+\sigma_1^2y_2}{D}\right|_{\mu=\overline{\mu}}=0\,,
\end{equation}
where $\overline{\mu}$ is the best fit value of $\mu$. Notice the
cancellation in the numerator of the correlated uncertainty
$\sigma_c$. This ``accidental'' cancellation in the offset case,
caused by the fact that $\sigma_c$ is the same for both $x_1$ and
$x_2$, can be regarded as the (misleading) evidence that the DAB only
takes place in the normalisation case. The solution to
Eq.~(\ref{eq:minchi2}) is:
\begin{equation}\label{eq:offcv}
\overline{\mu} = \frac{\sigma_2^2x_1+\sigma_1^2x_2}{\sigma_1^2+\sigma_2^2}=\frac{\frac{x_1}{\sigma_1^2}+\frac{x_2}{\sigma_2^2}}{\frac{1}{\sigma_1^2}+\frac{1}{\sigma_2^2}}\,,
\end{equation}
that fulfils the intuitive expectation that the best value of $\mu$
should be a weighted average of the central values weighted by the
inverse of the uncorrelated uncertainties squared. We can also compute
the uncertainty of $\overline{\mu}$, $\delta\mu$, by simply computing
the inverse of the second derivative of the $\chi^2$ at
$\mu=\overline{\mu}$:\footnote{Actually, the second derivative of the
  $\chi^2$ does not depend on $\mu$ and thus it is not necessary to
  set $\mu=\overline{\mu}$.}
\begin{equation}\label{eq:offcvunc}
  \delta\mu^2 = \left[\frac12\frac{d^2\chi^2}{d\mu^2}\right]_{\mu=\overline{\mu}}^{-1} = \frac {\sigma_1^2 \sigma_2^2+(\sigma_1^2+\sigma_2^2) \sigma_c^2}{\sigma_2^2+\sigma_1^2}=\frac {1+\frac{\sigma_c^2}{\sigma_1^2}+\frac{\sigma_c^2}{\sigma_2^2} }{\frac{1}{\sigma_1^2}+\frac{1}{\sigma_2^2}}\,.
\end{equation}

I now move to considering the normalisation case in which the
covariance matrix reads:
\begin{equation}
\mathbf{V}=
\begin{pmatrix}
\sigma_1^2+\delta_f^2x_1^2& \delta_f^2x_1x_2\\
\delta_f^2x_1x_2& \sigma_2^2+\delta_f^2x_2^2
\end{pmatrix}\,,
\end{equation}
whose inverse is:
\begin{equation}
\mathbf{V}^{-1}=\frac{1}{D}
\begin{pmatrix}
\sigma_2^2+\delta_f^2x_2^2& -\delta_f^2x_1x_2\\
-\delta_f^2x_1x_2& \sigma_1^2+\delta_f^2x_1^2
\end{pmatrix}\,,
\end{equation}
with
$D=\sigma_1^2\sigma_2^2+\delta_f^2(\sigma_1^2x_2^2+\sigma_2^2x_1^2)$. The
resulting $\chi^2$ is:
\begin{equation}
\chi^2=\frac{1}{D}\left[\sigma_2^2y_1^2+ \sigma_1^2y_2^2+\delta_f^2(x_2y_1 -x_1y_2)^2\right]\,.
\end{equation}
The minimum is computed as:
\begin{equation}
  \left.\frac{d\chi^2}{d\mu}\right|_{\mu=\overline{\mu}} = \left.2\frac{\sigma_2^2y_1+ \sigma_1^2y_2+\delta_f^2(x_2 -x_1)(x_2y_1 -x_1y_2)}{D}\right|_{\mu=\overline{\mu}}=0\,.
\end{equation}
This time, differently from the offset case, the correlated
uncertainties in the denominator do not cancel and the result of the
equation above is:
\begin{equation}\label{eq:normcv}
\overline{\mu} = \frac{\sigma_2^2x_1+\sigma_1^2x_2}{\sigma_1^2+\sigma_2^2+(\delta_f x_1-\delta_f x_2)^2}=\frac{\frac{x_1}{\sigma_1^2}+\frac{x_2}{\sigma_2^2}}{\frac{1}{\sigma_1^2}+\frac{1}{\sigma_2^2}+\left(\frac{\delta_f x_1-\delta_f x_2}{\sigma_1\sigma_2}\right)^2}\,,
\end{equation}
while:
\begin{equation}\label{eq:normcvunc}
  \delta\mu^2 = \left[\frac12\frac{d^2\chi^2}{d\mu^2}\right]_{\mu=\overline{\mu}}^{-1} = \frac{\sigma_1^2\sigma_2^2+\delta_f^2(\sigma_1^2x_2^2+\sigma_2^2x_1^2)}{\sigma_2^2+ \sigma_1^2+\delta_f^2(x_1 -x_2)^2}=\frac{1+\frac{\delta_f^2 x_1^2}{\sigma_1^2}+\frac{\delta_f^2 x_2^2}{\sigma_2^2}}{\frac{1}{\sigma_1^2}+ \frac{1}{\sigma_2^2}+\left(\frac{\delta_f x_1 -\delta_f x_2}{\sigma_1\sigma_2}\right)^2}\,.
\end{equation}
According to Ref.~\cite{DAgostini:1993arp}, the term proportional to
$\delta_f$ in the denominator of the r.h.s. of Eq.~(\ref{eq:normcv})
is a source of bias in that it deviates from the intuitive expectation
of Eq.~(\ref{eq:offcv}). Being this additional term positive, it tends
to diminish the best fit value.

I will now show that the additional term in the denominator of
Eq.~(\ref{eq:normcv}) is not a specific feature of normalisation
uncertainties. To see this, we consider the same data set of two data
points but where the data point $x_1$ is affected by the additive
uncertainty $\sigma_{c,1}$ and the point $x_2$ by the additive
uncertainty $\sigma_{c,2}$. According to the same procedure followed
above the final result for best-fit value and uncertainty of $\mu$ is:
\begin{equation}\label{eq:add2cv}
\overline{\mu} = \frac{\frac{x_1}{\sigma_1^2}+\frac{x_2}{\sigma_2^2}}{\frac{1}{\sigma_1^2}+\frac{1}{\sigma_2^2}+\left(\frac{\sigma_{c,1}-\sigma_{c,2}}{\sigma_1\sigma_2}\right)^2}\,,
\end{equation}
and:
\begin{equation}
  \delta\mu^2=\frac{1+\frac{\sigma_{c,1}^2}{\sigma_1^2}+\frac{\sigma_{c,2}^2}{\sigma_2^2}}{\frac{1}{\sigma_1^2}+ \frac{1}{\sigma_2^2}+\left(\frac{\sigma_{c,1} -\sigma_{c,2}}{\sigma_1\sigma_2}\right)^2}\,.
\end{equation}
These equations are (somewhat expectedly) equal to
Eqs.~(\ref{eq:normcv}) and~(\ref{eq:normcvunc}) with the only
difference that $\delta_fx_i$ is replaced by
$\sigma_{c,i}$. Therefore, the DAB shows up also when additive
uncertainties are considered as long as they are different in absolute
value. Therefore, the DAB is not a prerogative of normalisation
uncertainties only. As expected, the equations above reduce to
Eqs.~(\ref{eq:offcv}) and~(\ref{eq:offcvunc}) when
$\sigma_{c,1}=\sigma_{c,2}=\sigma_{c}$.

Another important aspect of Eqs.~(\ref{eq:normcv})
and~(\ref{eq:normcvunc}) is that the term that causes the ``bias'' is
proportional to the difference $x_1-x_2$. Therefore, if the central
values happen to be the same that term vanishes and thus no bias is
present. This observation, along with the fact that the DAB is
potentially present also when additive uncertainties are considered,
leads us to a simple conclusion: the DAB is present in all cases
whenever there are correlated uncertainties different in absolute
value. This is trivially the case when one introduces different
correlated additive uncertainties, $\sigma_{c,1}$ and $\sigma_{c,2}$
with $\sigma_{c,1}\neq \sigma_{c,2}$. But this also the case in the
presence of a normalisation uncertainties when central values are
different ($x_1\neq x_2$). On the contrary, if
$\sigma_{c,1}= \sigma_{c,2}$ in the additive case or $x_1=x_2$ in the
multiplicative case, no ``bias'' is present. The conclusion is that
the very distinction between additive and multiplicative uncertainties
is immaterial.

Now that we have established that the DAB is not a prerogative of
normalisation uncertainties only, I will try to show that this is in
fact not a bias but a manifestation of correlated uncertainties that,
under the assumption that experimental uncertainties are faithfully
estimated, needs not be corrected. The argument is based on the fact
that, in the presence of correlations, a visual comparison between
data and best fit value is meaningful only when including the
systematic shifts.




\begin{thebibliography}{alp}

%\cite{Ball:2009qv}
\bibitem{Ball:2009qv}
  R.~D.~Ball {\it et al.} [NNPDF Collaboration],
  %``Fitting Parton Distribution Data with Multiplicative Normalization Uncertainties,''
  JHEP {\bf 1005} (2010) 075
  doi:10.1007/JHEP05(2010)075
  [arXiv:0912.2276 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP05(2010)075;%%
  %84 citations counted in INSPIRE as of 13 May 2018

%\cite{DAgostini:1993arp}
\bibitem{DAgostini:1993arp}
  G.~D'Agostini,
  %``On the use of the covariance matrix to fit correlated data,''
  Nucl.\ Instrum.\ Meth.\ A {\bf 346} (1994) 306.
  doi:10.1016/0168-9002(94)90719-6
  %%CITATION = doi:10.1016/0168-9002(94)90719-6;%%
  %146 citations counted in INSPIRE as of 13 May 2018

%\cite{Ball:2012wy}
\bibitem{Ball:2012wy}
  R.~D.~Ball {\it et al.},
  %``Parton Distribution Benchmarking with LHC Data,''
  JHEP {\bf 1304} (2013) 125
  doi:10.1007/JHEP04(2013)125
  [arXiv:1211.5142 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP04(2013)125;%%
  %102 citations counted in INSPIRE as of 13 May 2018

%\cite{Boughezal:2017nla}
\bibitem{Boughezal:2017nla}
  R.~Boughezal, A.~Guffanti, F.~Petriello and M.~Ubiali,
  %``The impact of the LHC Z-boson transverse momentum data on PDF determinations,''
  JHEP {\bf 1707} (2017) 130
  doi:10.1007/JHEP07(2017)130
  [arXiv:1705.00343 [hep-ph]].
  %%CITATION = doi:10.1007/JHEP07(2017)130;%%
  %19 citations counted in INSPIRE as of 13 May 2018

\bibitem{wiki:xxx}
    Wikipedia contributors,
    Chi-square distribution --- {Wikipedia}{,} The Free Encyclopedia,
    2020.
   % \url{https://en.wikipedia.org/w/index.php?title=Chi-square_distribution&oldid=979585636}

%\cite{Ball:2008by}
\bibitem{Ball:2008by}
R.~D.~Ball \textit{et al.} [NNPDF],
%``A Determination of parton distributions with faithful uncertainty estimation,''
Nucl. Phys. B \textbf{809} (2009), 1-63
[erratum: Nucl. Phys. B \textbf{816} (2009), 293]
doi:10.1016/j.nuclphysb.2008.09.037
[arXiv:0808.1231 [hep-ph]].
%286 citations counted in INSPIRE as of 24 Oct 2020

%\cite{Ball:2014uwa}
\bibitem{Ball:2014uwa}
R.~D.~Ball \textit{et al.} [NNPDF],
%``Parton distributions for the LHC Run II,''
JHEP \textbf{04} (2015), 040
doi:10.1007/JHEP04(2015)040
[arXiv:1410.8849 [hep-ph]].
%2322 citations counted in INSPIRE as of 24 Oct 2020

%\cite{Ball:2009qv}
\bibitem{Ball:2009qv}
R.~D.~Ball \textit{et al.} [NNPDF],
%``Fitting Parton Distribution Data with Multiplicative Normalization Uncertainties,''
JHEP \textbf{05} (2010), 075
doi:10.1007/JHEP05(2010)075
[arXiv:0912.2276 [hep-ph]].
%115 citations counted in INSPIRE as of 29 Oct 2020

\end{thebibliography}


\end{document}
